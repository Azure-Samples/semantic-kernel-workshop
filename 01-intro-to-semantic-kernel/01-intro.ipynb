{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Semantic Kernel Introduction\n",
    "\n",
    "### Overview of Semantic Kernel (SK) and Its Importance\n",
    "\n",
    "**Semantic Kernel** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is *lightweight* and *modular*, designed for **enterprise-grade solutions** with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and **future-proof** – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build **robust, scalable AI applications** that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- **Bridging AI and Code**: SK combines natural language **prompts** with your **existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI *intends* and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK **plugins**. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions **modular and extensible**.\n",
    "- **Enterprise-ready**: SK includes support for **security, observability, and compliance** (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining *what* you want the AI to do (skills, prompts) and SK handles *how* to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to **automatically call functions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Services and Core Components of SK\n",
    "\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The `Kernel` holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "  - *Chat Models*: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "  - *Embedding Models*: for converting text to vector embeddings (used in memory/search).\n",
    "  - *Other Modalities*: connectors for images, speech, etc., if needed.\n",
    "  \n",
    "  You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantic-kernel python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env', override=True)\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "        endpoint=base_url,    \n",
    "        deployment_name=deployment_name,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully created a kernel and added a chat completion service.\n",
    "\n",
    "Similarly, you can add an embedding generation service via `kernel.add_service(text_embedding)` if performing semantic memory searches. But don't worry we will dive into this at a later stage.\n",
    "\n",
    "---\n",
    "\n",
    "### Functions and Plugins in SK\n",
    "\n",
    "**Functions** in Semantic Kernel are the actions that the AI can perform. They come in two types:\n",
    "\n",
    "- **Semantic Functions**: Backed by a prompt and LLM. For example, a function `TranslateToFrench` might use the prompt `\"Translate this to French: {{$input}}\"`.\n",
    "- **Native Functions**: Backed by code. For example, a function `SendEmail(to, subject, body)` that uses an API to send an email.\n",
    "\n",
    "These functions are typically grouped into **Plugins** (or \"Skills\"). Grouping functions into plugins helps manage and control which capabilities are exposed to the AI.\n",
    "\n",
    "**Using Plugins/Functions**: Once registered with the kernel (via `kernel.add_function` or `kernel.add_plugin`), functions become available for invocation. They can be called directly in code via `kernel.invoke(function, input)`, or the AI model can automatically choose to invoke them as needed.\n",
    "\n",
    "SK’s plugin system is highly flexible:\n",
    "- You can load **OpenAPI** specifications or API endpoints as plugins.\n",
    "- Plugins can be shared across projects, allowing organizations to build a library of useful AI plugins.\n",
    "\n",
    "---\n",
    "\n",
    "You can add plugins to the Kernel in various ways:\n",
    "\n",
    "- **Inline Definition**: Define a prompt or function in code and register it.\n",
    "- **From Files or Classes**: Load plugins from directories or Python classes decorated appropriately.\n",
    "\n",
    "For example, to add a simple **semantic function** inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel is an open-source toolkit enabling easy integration of AI models into C#, Python, or Java codebases for building AI agents and enterprise-grade solutions.\n"
     ]
    }
   ],
   "source": [
    "# Define a semantic function (prompt) to generate a TL;DR summary\n",
    "prompt_template = \"{{$input}}\\n\\nTL;DR in one sentence:\"\n",
    "\n",
    "summarize_fn = kernel.add_function(\n",
    "    prompt=prompt_template, \n",
    "    function_name=\"tldr\", \n",
    "    plugin_name=\"Summarizer\",\n",
    "    max_tokens=50)\n",
    "\n",
    "# Use the function\n",
    "long_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(summarize_fn, input=long_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a semantic function, taken a text and a target language it can provide the translation of this text in the target laanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel est un kit de développement léger et open-source qui vous permet de créer facilement des agents d'IA et d'intégrer les derniers modèles d'IA dans votre code en C#, Python ou Java. Il sert de middleware efficace permettant une livraison\n"
     ]
    }
   ],
   "source": [
    "# Define a semantic function (prompt) to generate a TL;DR summary\n",
    "prompt_template = \"{{$input}}\\n\\nTranslate this into {{$target_lang}}:\"\n",
    "\n",
    "translate_fn = kernel.add_function(\n",
    "    prompt=prompt_template, \n",
    "    function_name=\"translator\", \n",
    "    plugin_name=\"Translator\",\n",
    "    max_tokens=50)\n",
    "\n",
    "# Use the function\n",
    "text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(translate_fn, input=text, target_lang=\"French\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now if we want to build a Plugin with a set of **native functions** we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "   id: int\n",
    "   name: str\n",
    "   is_on: bool | None\n",
    "   brightness: int | None\n",
    "   hex: str | None\n",
    "\n",
    "class LightsPlugin:\n",
    "   def __init__(self, lights: list[LightModel]):\n",
    "      self.lights = lights\n",
    "   \n",
    "\n",
    "   @kernel_function\n",
    "   async def get_lights(self) -> List[LightModel]:\n",
    "      \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "      return self.lights\n",
    "\n",
    "   @kernel_function\n",
    "   async def get_state(\n",
    "      self,\n",
    "      id: Annotated[int, \"The ID of the light\"]\n",
    "   ) -> Optional[LightModel]:\n",
    "      \"\"\"Gets the state of a particular light.\"\"\"\n",
    "      for light in self.lights:\n",
    "         if light[\"id\"] == id:\n",
    "               return light\n",
    "      return None\n",
    "\n",
    "   @kernel_function\n",
    "   async def change_state(\n",
    "      self,\n",
    "      id: Annotated[int, \"The ID of the light\"],\n",
    "      new_state: LightModel\n",
    "   ) -> Optional[LightModel]:\n",
    "      \"\"\"Changes the state of the light.\"\"\"\n",
    "      for light in self.lights:\n",
    "         if light[\"id\"] == id:\n",
    "               light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "               light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "               light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "               return light\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True), KernelParameterMetadata(name='new_state', description=None, default_value=None, type_='LightModel', is_required=True, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121ae0550>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121aefce0>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x121bc5400>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state.', parameters=[], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='list[LightModel]', is_required=True, type_object=<class 'list'>, schema_data={'type': 'array'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121aefe10>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121b7b410>, method=<bound method LightsPlugin.get_lights of <__main__.LightsPlugin object at 0x121bc5400>>, stream_method=None), 'get_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_state', plugin_name='Lights', description='Gets the state of a particular light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121b999d0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x121b997b0>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x121bc5400>>, stream_method=None)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies for the plugin\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "\n",
    "\n",
    "kernel.add_plugin(\n",
    "   plugin=plugin,\n",
    "   plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The Table Lamp is already turned on. If you need any adjustments or further actions, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"Please turn on the lamp\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Function Calling\n",
    "\n",
    "One of the most powerful features of Semantic Kernel is its ability to **automatically orchestrate multi-step operations** by calling multiple functions in sequence. In this example, a **LightsPlugin** is defined with three asynchronous native functions:\n",
    "\n",
    "- **get_lights()**: Retrieves the list of lights along with their current states.\n",
    "- **get_state(id)**: Returns the state of a specific light, given its ID.\n",
    "- **change_state(id, new_state)**: Changes the state of a specified light to a new state (for example, turning it on, adjusting brightness, or changing its color).\n",
    "\n",
    "**How It Works**:\n",
    "- The kernel exposes all registered functions to the AI model.\n",
    "- When a user issues a command like *\"Turn on all the lights and give me their final state,\"* the AI model analyzes the request and plans the necessary steps:\n",
    "  1. Call **get_lights()** to retrieve all available lights.\n",
    "  2. For each light, invoke **change_state()** with a new state (e.g., setting `\"is_on\": True`).\n",
    "  3. Optionally, call **get_state()** for each light to confirm the updated status.\n",
    "- The kernel then returns a comprehensive result that reflects the final state of each light.\n",
    "\n",
    "**Example Scenario**:\n",
    "- **User Query**: *\"Turn on all the lights and tell me their status.\"*\n",
    "- **Step 1**: The system calls `LightsPlugin.get_lights()` to fetch the current list of lights.\n",
    "- **Step 2**: It iterates over the list and calls `LightsPlugin.change_state(id, new_state)` to turn each light on.\n",
    "- **Step 3**: Finally, it may call `LightsPlugin.get_state(id)` for each light to confirm the changes.\n",
    "- The final output, including the updated state for each light, is returned to the user.\n",
    "\n",
    "This automatic orchestration simplifies the management of multi-step tasks, enabling the AI to autonomously plan and execute function calls without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Exercise\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters in Semantic Kernel\n",
    "\n",
    "Filters provide a layer of control and visibility over function execution, ensuring responsible AI practices and enterprise-grade security. They allow you to:\n",
    "\n",
    "- **Validate Permissions:**  \n",
    "  For example, a filter can check user permissions before initiating an approval flow.\n",
    "\n",
    "- **Intercept Function Execution:**  \n",
    "  - **Function Invocation Filter:**  \n",
    "    Runs every time a function is called; it can access function details, handle exceptions, override results (e.g., for caching or responsible AI), or retry on failure.\n",
    "  - **Prompt Render Filter:**  \n",
    "    Triggered before a prompt is rendered; it allows you to view or modify the prompt and even override the result to prevent submission.\n",
    "  - **Auto Function Invocation Filter:**  \n",
    "    Works within automatic function calling, providing additional context (like chat history and iteration counters) and can terminate the process early if needed.\n",
    "\n",
    "Each filter receives a context object with execution details and must call the next delegate (or callback) to continue the execution chain. Filters can be registered either by using the `add_filter` method on the Kernel or via the `@kernel.filter` decorator.\n",
    "\n",
    "\n",
    "One of the things that I would like to improve in our plugin implementation is to add debugging, that way I can integrate with external systems for auditing purposes\n",
    "\n",
    "Lets implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Awaitable, Callable\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "async def logger_filter(context: FunctionInvocationContext, next: Callable[[FunctionInvocationContext], Awaitable[None]]) -> None:\n",
    "    logger.info(f\"FunctionInvoking - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "    await next(context)\n",
    "\n",
    "    logger.info(f\"FunctionInvoked - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "kernel.add_filter('function_invocation', logger_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:33:14,725 - INFO - HTTP Request: POST https://aoai-sweden-gbb-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:33:14,729 - INFO - OpenAI usage: CompletionUsage(completion_tokens=117, prompt_tokens=903, total_tokens=1020, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-02-27 14:33:14,731 - INFO - processing 3 tool calls in parallel.\n",
      "2025-02-27 14:33:14,732 - INFO - Calling Lights-change_state function with args: {\"id\": 1, \"new_state\": {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": true}}\n",
      "2025-02-27 14:33:14,734 - INFO - Function Lights-change_state invoking.\n",
      "2025-02-27 14:33:14,735 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,736 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,736 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,737 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,737 - INFO - Function Lights-change_state succeeded.\n",
      "2025-02-27 14:33:14,738 - INFO - Function completed. Duration: 0.003135s\n",
      "2025-02-27 14:33:14,739 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": true}}\n",
      "2025-02-27 14:33:14,740 - INFO - Function Lights-change_state invoking.\n",
      "2025-02-27 14:33:14,741 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,742 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,742 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,743 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,743 - INFO - Function Lights-change_state succeeded.\n",
      "2025-02-27 14:33:14,744 - INFO - Function completed. Duration: 0.002975s\n",
      "2025-02-27 14:33:14,745 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": true}}\n",
      "2025-02-27 14:33:14,745 - INFO - Function Lights-change_state invoking.\n",
      "2025-02-27 14:33:14,746 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,746 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-02-27 14:33:14,746 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,746 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-02-27 14:33:14,747 - INFO - Function Lights-change_state succeeded.\n",
      "2025-02-27 14:33:14,747 - INFO - Function completed. Duration: 0.001814s\n",
      "2025-02-27 14:33:17,077 - INFO - HTTP Request: POST https://aoai-sweden-gbb-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:33:17,079 - INFO - OpenAI usage: CompletionUsage(completion_tokens=121, prompt_tokens=1136, total_tokens=1257, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > All the lamps have been turned on. Here is their current state:\n",
      "\n",
      "1. **Table Lamp**: \n",
      "   - State: On \n",
      "   - Brightness: 100 \n",
      "   - Color: #FF0000\n",
      "\n",
      "2. **Porch Light**: \n",
      "   - State: On \n",
      "   - Brightness: 50 \n",
      "   - Color: #00FF00\n",
      "\n",
      "3. **Chandelier**: \n",
      "   - State: On \n",
      "   - Brightness: 75 \n",
      "   - Color: #0000FF\n",
      "\n",
      "If you need any further assistance, just let me know!\n"
     ]
    }
   ],
   "source": [
    "history.add_user_message(\"Please turn on all the lamps and give me their final state\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters in SK and Their Use Cases\n",
    "\n",
    "To summarize, in any AI application, it’s important to control input/output and function execution for security, privacy, and correctness. **Filters** in Semantic Kernel act as middleware or interceptors in the execution pipeline.\n",
    "\n",
    "**Use Cases for Filters**:\n",
    "- **Security/Policy**: Prevent sensitive data from being sent to the AI.\n",
    "- **Validation**: Check function arguments before execution.\n",
    "- **Error Handling**: Catch exceptions and provide default results.\n",
    "- **Logging/Monitoring**: Log each function call and its response.\n",
    "- **Post-processing**: Modify outputs before they’re returned to the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context**: The Kernel maintains a context that holds variables (including memory) and the history of the conversation. This context is essential for chaining function calls and maintaining continuity.\n",
    "- **Planner (Plans)**: SK includes a *planner* that can take a high-level goal and figure out a sequence of function calls to achieve it. This is useful for dynamic task orchestration.\n",
    "- **Memory**: SK provides a **semantic memory** store for long-term information. Memory is typically backed by a **vector database** or search index. This enables Retrieval-Augmented Generation (RAG) scenarios by allowing the AI to recall relevant information from stored data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
