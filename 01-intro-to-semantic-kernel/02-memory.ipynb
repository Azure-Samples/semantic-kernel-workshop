{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Memory in Semantic Kernel\n",
    "\n",
    "In AI applications, memory is crucial for creating contextual, personalized experiences. Semantic Kernel provides powerful memory management capabilities that allow your AI applications to:\n",
    "\n",
    "- Remember facts and knowledge over time\n",
    "- Find information based on meaning rather than exact matches\n",
    "- Use previous context in ongoing conversations\n",
    "- Implement Retrieval-Augmented Generation (RAG) patterns\n",
    "\n",
    "\n",
    "\n",
    "This notebook explores how to implement and use memory capabilities in Semantic Kernel applications. \n",
    "\n",
    "\n",
    "Let's visualize how memory fits into the Semantic Kernel architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"semantic-kernel[azure]\" mermaid-py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mermaid as md\n",
    "from mermaid.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg id=\"mermaid-svg\" width=\"100%\" xmlns=\"http://www.w3.org/2000/svg\" class=\"flowchart\" style=\"max-width: 716.9140625px;\" viewBox=\"0 0 716.9140625 590\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><style xmlns=\"http://www.w3.org/1999/xhtml\">@import url(\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\");</style><style>#mermaid-svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg .error-icon{fill:#552222;}#mermaid-svg .error-text{fill:#552222;stroke:#552222;}#mermaid-svg .edge-thickness-normal{stroke-width:1px;}#mermaid-svg .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-svg .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg .marker{fill:#333333;stroke:#333333;}#mermaid-svg .marker.cross{stroke:#333333;}#mermaid-svg svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg p{margin:0;}#mermaid-svg .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#333;}#mermaid-svg .cluster-label text{fill:#333;}#mermaid-svg .cluster-label span{color:#333;}#mermaid-svg .cluster-label span p{background-color:transparent;}#mermaid-svg .label text,#mermaid-svg span{fill:#333;color:#333;}#mermaid-svg .node rect,#mermaid-svg .node circle,#mermaid-svg .node ellipse,#mermaid-svg .node polygon,#mermaid-svg .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg .rough-node .label text,#mermaid-svg .node .label text,#mermaid-svg .image-shape .label,#mermaid-svg .icon-shape .label{text-anchor:middle;}#mermaid-svg .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#mermaid-svg .rough-node .label,#mermaid-svg .node .label,#mermaid-svg .image-shape .label,#mermaid-svg .icon-shape .label{text-align:center;}#mermaid-svg .node.clickable{cursor:pointer;}#mermaid-svg .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#mermaid-svg .arrowheadPath{fill:#333333;}#mermaid-svg .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#mermaid-svg .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#mermaid-svg .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#mermaid-svg .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#mermaid-svg .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg .cluster text{fill:#333;}#mermaid-svg .cluster span{color:#333;}#mermaid-svg div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#mermaid-svg rect.text{fill:none;stroke-width:0;}#mermaid-svg .icon-shape,#mermaid-svg .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#mermaid-svg .icon-shape p,#mermaid-svg .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#mermaid-svg .icon-shape rect,#mermaid-svg .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#mermaid-svg :root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}</style><g><marker id=\"mermaid-svg_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"mermaid-svg_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"mermaid-svg_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"mermaid-svg_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"mermaid-svg_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"mermaid-svg_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path d=\"M276.93,62L276.93,66.167C276.93,70.333,276.93,78.667,276.93,86.333C276.93,94,276.93,101,276.93,104.5L276.93,108\" id=\"L_A_B_0\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M223.805,152.584L198.766,158.987C173.727,165.39,123.648,178.195,98.609,188.097C73.57,198,73.57,205,73.57,208.5L73.57,212\" id=\"L_B_C_1\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M276.93,166L276.93,170.167C276.93,174.333,276.93,182.667,276.93,190.333C276.93,198,276.93,205,276.93,208.5L276.93,212\" id=\"L_B_D_2\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M330.055,153.237L353.539,159.531C377.023,165.825,423.992,178.412,447.477,188.206C470.961,198,470.961,205,470.961,208.5L470.961,212\" id=\"L_B_E_3\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M212.941,270L203.066,274.167C193.191,278.333,173.441,286.667,163.566,294.333C153.691,302,153.691,309,153.691,312.5L153.691,316\" id=\"L_D_F_4\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M340.919,270L350.794,274.167C360.669,278.333,380.418,286.667,390.293,294.333C400.168,302,400.168,309,400.168,312.5L400.168,316\" id=\"L_D_G_5\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M340.558,374L331.359,378.167C322.16,382.333,303.761,390.667,294.562,398.333C285.363,406,285.363,413,285.363,416.5L285.363,420\" id=\"L_G_H_6\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M459.778,374L468.977,378.167C478.176,382.333,496.574,390.667,505.774,398.333C514.973,406,514.973,413,514.973,416.5L514.973,420\" id=\"L_G_I_7\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M459.401,478L450.825,482.167C442.249,486.333,425.097,494.667,416.521,502.333C407.945,510,407.945,517,407.945,520.5L407.945,524\" id=\"L_I_J_8\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/><path d=\"M570.545,478L579.12,482.167C587.696,486.333,604.848,494.667,613.424,502.333C622,510,622,517,622,520.5L622,524\" id=\"L_I_K_9\" class=\" edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\"\" marker-end=\"url(#mermaid-svg_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel \"></span></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default  \" id=\"flowchart-A-0\" transform=\"translate(276.9296875, 35)\"><rect class=\"basic label-container\" style=\"\" x=\"-69.140625\" y=\"-27\" width=\"138.28125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-39.140625, -12)\"><rect/><foreignObject width=\"78.28125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Application</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-B-1\" transform=\"translate(276.9296875, 139)\"><rect class=\"basic label-container\" style=\"\" x=\"-53.125\" y=\"-27\" width=\"106.25\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-23.125, -12)\"><rect/><foreignObject width=\"46.25\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Kernel</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-C-3\" transform=\"translate(73.5703125, 243)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.5703125\" y=\"-27\" width=\"131.140625\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-35.5703125, -12)\"><rect/><foreignObject width=\"71.140625\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>AI Models</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-D-5\" transform=\"translate(276.9296875, 243)\"><rect class=\"basic label-container\" style=\"fill:#f9d5e5 !important;stroke:#333 !important;stroke-width:2px !important\" x=\"-87.7890625\" y=\"-27\" width=\"175.578125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-57.7890625, -12)\"><rect/><foreignObject width=\"115.578125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Memory System</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-E-7\" transform=\"translate(470.9609375, 243)\"><rect class=\"basic label-container\" style=\"\" x=\"-56.2421875\" y=\"-27\" width=\"112.484375\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-26.2421875, -12)\"><rect/><foreignObject width=\"52.484375\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Plugins</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-F-9\" transform=\"translate(153.69140625, 347)\"><rect class=\"basic label-container\" style=\"\" x=\"-98.8984375\" y=\"-27\" width=\"197.796875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-68.8984375, -12)\"><rect/><foreignObject width=\"137.796875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Short-term Memory</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-G-11\" transform=\"translate(400.16796875, 347)\"><rect class=\"basic label-container\" style=\"\" x=\"-97.578125\" y=\"-27\" width=\"195.15625\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-67.578125, -12)\"><rect/><foreignObject width=\"135.15625\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Long-term Memory</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-H-13\" transform=\"translate(285.36328125, 451)\"><rect class=\"basic label-container\" style=\"\" x=\"-99.375\" y=\"-27\" width=\"198.75\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-69.375, -12)\"><rect/><foreignObject width=\"138.75\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Vector Embeddings</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-I-15\" transform=\"translate(514.97265625, 451)\"><rect class=\"basic label-container\" style=\"\" x=\"-80.234375\" y=\"-27\" width=\"160.46875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-50.234375, -12)\"><rect/><foreignObject width=\"100.46875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Memory Store</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-J-17\" transform=\"translate(407.9453125, 555)\"><rect class=\"basic label-container\" style=\"\" x=\"-77.140625\" y=\"-27\" width=\"154.28125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-47.140625, -12)\"><rect/><foreignObject width=\"94.28125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Volatile Store</p></span></div></foreignObject></g></g><g class=\"node default  \" id=\"flowchart-K-19\" transform=\"translate(622, 555)\"><rect class=\"basic label-container\" style=\"\" x=\"-86.9140625\" y=\"-27\" width=\"173.828125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-56.9140625, -12)\"><rect/><foreignObject width=\"113.828125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel \"><p>Persistent Store</p></span></div></foreignObject></g></g></g></g></g></svg>"
      ],
      "text/plain": [
       "<mermaid.__main__.Mermaid at 0x10c8c42f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = Graph('Sequence-diagram',\"\"\"\n",
    "graph TD\n",
    "    A[Application] --> B[Kernel]\n",
    "    B --> C[AI Models]\n",
    "    B --> D[Memory System]\n",
    "    B --> E[Plugins]\n",
    "    D --> F[Short-term Memory]\n",
    "    D --> G[Long-term Memory]\n",
    "    G --> H[Vector Embeddings]\n",
    "    G --> I[Memory Store]\n",
    "    I --> J[Volatile Store]\n",
    "    I --> K[Persistent Store]\n",
    "    style D fill:#f9d5e5,stroke:#333,stroke-width:2px\n",
    "\"\"\")\n",
    "md.Mermaid(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Memory\n",
    "In SK, **memory** refers to the storage and recall of information the AI has learned or been provided. There are two primary forms of memory:\n",
    "\n",
    "### Semantic Memory (Long-term)\n",
    "- This is usually an **external vector store** that holds **embeddings of text**, allowing the AI to store facts or documents and later retrieve them by **semantic similarity**.\n",
    "- SK provides **Memory Connectors** to various vector databases (like **Azure Cognitive Search, Pinecone, Qdrant**, etc.) via a common interface.\n",
    "- By using a memory store, you can implement the **retrieval** part of **RAG**: store chunks of knowledge and fetch relevant pieces at query time.\n",
    "- We’ll see how to add and use such memory in our chatbot.\n",
    "\n",
    "### Conversation History (Short-term Memory)\n",
    "- SK also manages the **immediate dialogue context** with a **Chat History object** for multi-turn conversations.\n",
    "- This ensures the AI remembers prior user queries and its own responses, maintaining context across turns.\n",
    "- We will leverage this to keep the conversation coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.0\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/Dev/ip/semantic-kernel-workshop/.venv/lib/python3.13/site-packages/google_crc32c/__init__.py:29: RuntimeWarning: As the c extension couldn't be imported, `google-crc32c` is using a pure python implementation that is significantly slower. If possible, please configure a c build environment and compile the extension\n",
      "  warnings.warn(_SLOW_CRC32C_WARNING, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Create the embedding service\n",
    "embedding_service = AzureTextEmbedding(\n",
    "    endpoint=base_url,\n",
    "    deployment_name=embedding_deployment_name,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates an embedding service that connects to Azure OpenAI. This service will convert text into vector embeddings which are numerical representations that capture semantic meaning. The environment variables should be set in your `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize our semantic memory system with:\n",
    "\n",
    "- A VolatileMemoryStore - an in-memory vector database (data will be lost when your session ends)\n",
    "- The embedding service we created earlier, which will generate vector embeddings for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"generic\"\n",
    "\n",
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await memory.save_information(collection=collection_id, id=\"info1\", text=\"Your budget for 2024 is $100,000\")\n",
    "    await memory.save_information(collection=collection_id, id=\"info2\", text=\"Your savings from 2023 are $50,000\")\n",
    "    await memory.save_information(collection=collection_id, id=\"info3\", text=\"Your investments are $80,000\")\n",
    "\n",
    "\n",
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function adds information to our memory store. Each memory item consists of:\n",
    "\n",
    "- `collection`: A namespace for organizing related memories (like a database table)\n",
    "- `id`: A unique identifier for retrieving specific memories\n",
    "- `text`: The actual information to store\n",
    "\n",
    "\n",
    "When we save information, Semantic Memory:\n",
    "\n",
    "1. Generates an embedding vector for the text\n",
    "2. Stores both the text and its vector in the memory store\n",
    "3. Associates it with the given ID and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(memory: SemanticTextMemory) -> None:\n",
    "    questions = [\n",
    "        \"What is my budget for 2024?\",\n",
    "        \"What are my savings from 2023?\",\n",
    "        \"What are my investments?\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        result = await memory.search(collection_id, question)\n",
    "        print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is my budget for 2024?\n",
      "Answer: Your budget for 2024 is $100,000\n",
      "\n",
      "Question: What are my savings from 2023?\n",
      "Answer: Your savings from 2023 are $50,000\n",
      "\n",
      "Question: What are my investments?\n",
      "Answer: Your investments are $80,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await search_memory_examples(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How does semantic search work?\n",
    "\n",
    "1. We provide a natural language query (e.g., \"What is my budget for 2024?\")\n",
    "2. The memory system:\n",
    "   - Converts the query to a vector embedding\n",
    "   - Compares this vector against stored embeddings using cosine similarity\n",
    "   - Returns the closest matching results\n",
    "   \n",
    "The search works even if the query doesn't exactly match the stored text, as it finds semantically similar content.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Adding and Retrieving Custom Memories\n",
    "\n",
    "Try adding your own information to the memory and retrieving it with semantic search.\n",
    "\n",
    "1. Create a new collection called \"personal\"\n",
    "2. Add at least three facts about a fictional person\n",
    "3. Search for those facts using natural language queries\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "```python\n",
    "# Create a new collection\n",
    "personal_collection = \"personal\"\n",
    "\n",
    "# Add information to memory\n",
    "async def add_personal_info(memory):\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact1\", text=\"John was born in Seattle in 1980\")\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact2\", text=\"John graduated from University of Washington in 2002\")\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact3\", text=\"John has two children named Alex and Sam\")\n",
    "\n",
    "await add_personal_info(memory)\n",
    "\n",
    "# Search for information\n",
    "questions = [\n",
    "    \"Where was John born?\",\n",
    "    \"When did John graduate college?\",\n",
    "    \"Does John have kids?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    result = await memory.search(personal_collection, question)\n",
    "    print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# Create a new collection\n",
    "\n",
    "# Add information to memory\n",
    "\n",
    "# Search for information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Setup\n",
    "\n",
    "This code creates a new Kernel instance and adds both:\n",
    "\n",
    "1. A chat completion service for generating responses\n",
    "2. The embedding service we created earlier for vector operations\n",
    "\n",
    "This configuration allows the kernel to generate text and work with vector embeddings in memory operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env', override=True)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "        endpoint=base_url,    \n",
    "        deployment_name=deployment_name,\n",
    "        api_key=api_key,\n",
    "        service_id='chat',\n",
    "    )\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# we also add the embedding service to the kernel\n",
    "kernel.add_service(embedding_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='TextMemoryPlugin', description=None, functions={'recall': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='recall', plugin_name='TextMemoryPlugin', description='Recall a fact from the long term memory', parameters=[KernelParameterMetadata(name='ask', description='The information to retrieve', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to retrieve'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to search for information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to search for information.'}, include_in_function_choices=True), KernelParameterMetadata(name='relevance', description='The relevance score, from 0.0 to 1.0; 1.0 means perfect match', default_value=0.75, type_='float', is_required=False, type_object=<class 'float'>, schema_data={'type': 'number', 'description': 'The relevance score, from 0.0 to 1.0; 1.0 means perfect match'}, include_in_function_choices=True), KernelParameterMetadata(name='limit', description='The maximum number of relevant memories to recall.', default_value=1, type_='int', is_required=False, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The maximum number of relevant memories to recall.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x10c8c6510>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x1690234d0>, method=<bound method TextMemoryPlugin.recall of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None), 'save': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='save', plugin_name='TextMemoryPlugin', description='Save information to semantic memory', parameters=[KernelParameterMetadata(name='text', description='The information to save.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to save.'}, include_in_function_choices=True), KernelParameterMetadata(name='key', description='The unique key to associate with the information.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The unique key to associate with the information.'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to save the information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to save the information.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='None', is_required=False, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x1690220d0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x168843820>, method=<bound method TextMemoryPlugin.save of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None)})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_service)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we:\n",
    "1. Create a `SemanticTextMemory` object with our in-memory store and embedding service\n",
    "2. Add the `TextMemoryPlugin` to the kernel, which provides memory-related functions\n",
    "\n",
    "The `TextMemoryPlugin` exposes memory operations to the kernel, allowing:\n",
    "- Semantic search through the `recall` function\n",
    "- Saving new information during conversations\n",
    "- Integration of memory capabilities into AI responses\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Now we set up a chat function that incorporates memory:\n",
    "\n",
    "1. We define a prompt template that:\n",
    "   - Gives the AI a role and instructions\n",
    "   - Uses the `{{recall '...'}}` syntax to search memory for relevant information\n",
    "   - Includes the user's request via `{{$request}}`\n",
    "\n",
    "2. We create a kernel function from this template\n",
    "\n",
    "The `{{recall 'query'}}` syntax tells Semantic Kernel to:\n",
    "1. Search the memory for information relevant to the query\n",
    "2. Insert the retrieved information into the prompt\n",
    "3. Let the AI use this information in its response\n",
    "\n",
    "This creates a chatbot that can reference previously stored financial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelFunction\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "\n",
    "async def setup_chat_with_memory(\n",
    "    kernel: Kernel,\n",
    "    service_id: str,\n",
    ") -> KernelFunction:\n",
    "    prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{recall 'budget by year'}} What is my budget for 2024?\n",
    "    - {{recall 'savings from previous year'}} What are my savings from 2023?\n",
    "    - {{recall 'investments'}} What are my investments?\n",
    "\n",
    "    {{$request}}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    prompt_template_config = PromptTemplateConfig(\n",
    "        template=prompt,\n",
    "        execution_settings={\n",
    "            service_id: kernel.get_service(service_id).get_prompt_execution_settings_class()(service_id=service_id)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return kernel.add_function(\n",
    "        function_name=\"chat_with_memory\",\n",
    "        plugin_name=\"chat\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating memory...\n",
      "Asking questions... (manually)\n",
      "Question: What is my budget for 2024?\n",
      "Answer: Your budget for 2024 is $100,000\n",
      "\n",
      "Question: What are my savings from 2023?\n",
      "Answer: Your savings from 2023 are $50,000\n",
      "\n",
      "Question: What are my investments?\n",
      "Answer: Your investments are $80,000\n",
      "\n",
      "Setting up a chat (with memory!)\n",
      "Begin chatting (type 'exit' to exit):\n",
      "\n",
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try asking a question about your finances (i.e. \"talk to me about my finances\").\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating memory...\")\n",
    "await populate_memory(memory)\n",
    "\n",
    "print(\"Asking questions... (manually)\")\n",
    "await search_memory_examples(memory)\n",
    "\n",
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func = await setup_chat_with_memory(kernel, 'chat')\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try asking a question about your finances (i.e. \\\"talk to me about my finances\\\").\"\n",
    ")\n",
    "\n",
    "\n",
    "async def chat(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "    answer = await kernel.invoke(chat_func, request=user_input)\n",
    "    print(f\"ChatBot:> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is my budget for 2024?\n",
      "ChatBot:> Your budget for 2024 is $100,000.\n"
     ]
    }
   ],
   "source": [
    "await chat(\"What is my budget for 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: talk to me about my finances\n",
      "ChatBot:> Based on the information you've shared from previous conversations, here's a brief overview of your financial situation:\n",
      "\n",
      "1. **Budget for 2024**: You have a budget of $100,000 set for the year. This indicates the amount you have planned to allocate or spend across various needs and goals.\n",
      "\n",
      "2. **Savings from 2023**: You have accumulated savings of $50,000 from the previous year. This amount could be set aside for future financial security, emergencies, or specific goals.\n",
      "\n",
      "3. **Investments**: You currently hold investments totaling $80,000. This might be invested in various assets like stocks, bonds, real estate, or other financial vehicles meant to grow over time.\n",
      "\n",
      "Considering this information, here are a few things you might focus on or discuss:\n",
      "\n",
      "- **Financial goals**: What specific goals do you have for your budget, savings, and investments? Are you planning any significant purchases, further investments, or building a stronger emergency fund?\n",
      "\n",
      "- **Investment strategy**: How are your investments performing? Are you satisfied with their growth, or are you considering changes? Depending on market conditions, maintaining or adjusting your portfolio could be beneficial.\n",
      "\n",
      "- **Savings growth**: How do you plan to increase your savings in 2024? Setting a monthly savings goal and sticking to it can contribute to substantial growth over time.\n",
      "\n",
      "- **Budget management**: Are there particular areas where you think you might need to adjust your budget? It's always good to review your budget periodically to ensure you're staying on track with financial goals.\n",
      "\n",
      "If you need help in any of these areas or have specific questions, let me know!\n"
     ]
    }
   ],
   "source": [
    "await chat(\"talk to me about my finances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# we also add the embedding service to the kernel\n",
    "kernel.add_service(embedding_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) with Self-Critique\n",
    "\n",
    "This section demonstrates a powerful pattern combining memory retrieval with response evaluation:\n",
    "\n",
    "1. **RAG Prompt**: This prompt template:\n",
    "   - Retrieves information from memory relevant to the user's question\n",
    "   - Provides this context to the AI\n",
    "   - Uses the context to generate an informed answer\n",
    "\n",
    "2. **Self-Critique**: This second prompt evaluates the quality of RAG responses:\n",
    "   - Takes the original question, retrieved context, and generated answer\n",
    "   - Classifies the answer as \"Grounded\", \"Ungrounded\", or \"Unclear\"\n",
    "   - Helps ensure responses are properly using retrieved information\n",
    "\n",
    "This pattern creates more reliable AI responses by:\n",
    "- Providing relevant facts from memory\n",
    "- Checking if responses properly use this information\n",
    "- Identifying when responses make claims beyond available information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating memory...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search.azure_ai_search_settings import AzureAISearchSettings\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.memory import SemanticTextMemory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"generic\"\n",
    "\n",
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the ACS semantic memory\n",
    "    await memory.save_information(COLLECTION_NAME, id=\"info1\", text=\"My name is Andrea\")\n",
    "    await memory.save_information(COLLECTION_NAME, id=\"info2\", text=\"I currently work as a tour guide\")\n",
    "    await memory.save_information(COLLECTION_NAME, id=\"info3\", text=\"I've been living in Seattle since 2005\")\n",
    "    await memory.save_information(\n",
    "        COLLECTION_NAME,\n",
    "        id=\"info4\",\n",
    "        text=\"I visited France and Italy five times since 2015\",\n",
    "    )\n",
    "    await memory.save_information(COLLECTION_NAME, id=\"info5\", text=\"My family is from New York\")\n",
    "\n",
    "azure_ai_search_settings = AzureAISearchSettings.create(endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"), api_key=os.getenv(\"AZURE_AI_SEARCH_API_KEY\"))\n",
    "vector_size = 1536\n",
    "\n",
    "\n",
    "acs_connector = AzureCognitiveSearchMemoryStore(\n",
    "    vector_size=vector_size,\n",
    "    search_endpoint=azure_ai_search_settings.endpoint,\n",
    "    admin_key=azure_ai_search_settings.api_key,\n",
    ")\n",
    "\n",
    "memory = SemanticTextMemory(storage=acs_connector, embeddings_generator=embedding_service)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "print(\"Populating memory...\")\n",
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Do I live in Seattle?\n",
      "Answer: Based on the information provided, yes, you have been living in Seattle since 2005.\n"
     ]
    }
   ],
   "source": [
    "\"It can give explicit instructions or say 'I don't know' if it does not have an answer.\"\n",
    "\n",
    "sk_prompt_rag = \"\"\"\n",
    "Assistant can have a conversation with you about any topic.\n",
    "\n",
    "Here is some background information about the user that you should use to answer the question below:\n",
    "{{ recall $user_input }}\n",
    "User: {{$user_input}}\n",
    "Assistant: \"\"\".strip()\n",
    "\n",
    "user_input = \"Do I live in Seattle?\"\n",
    "print(f\"Question: {user_input}\")\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat\")\n",
    "chat_func = kernel.add_function(\n",
    "    function_name=\"rag\", plugin_name=\"RagPlugin\", prompt=sk_prompt_rag, prompt_execution_settings=req_settings\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(user_input)\n",
    "\n",
    "answer = await kernel.invoke(\n",
    "    chat_func,\n",
    "    user_input=user_input,\n",
    "    chat_history=chat_history,\n",
    ")\n",
    "chat_history.add_assistant_message(str(answer))\n",
    "print(f\"Answer: {str(answer).strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the information provided, yes, you have been living in Seattle since 2005.\n",
      "The answer was Grounded\n",
      "--------------------------------------------------\n",
      "   Let's pretend the answer was wrong...\n",
      "Answer: Based on the information provided, yes, you have been living in Seattle since 2005.\n",
      "The answer was Ungrounded\n",
      "--------------------------------------------------\n",
      "   Let's pretend the answer is not related...\n",
      "Answer: Based on the information provided, yes, you have been living in Seattle since 2005.\n",
      "The answer was Unclear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sk_prompt_rag_sc = \"\"\"\n",
    "You will get a question, background information to be used with that question and a answer that was given.\n",
    "You have to answer Grounded or Ungrounded or Unclear.\n",
    "Grounded if the answer is based on the background information and clearly answers the question.\n",
    "Ungrounded if the answer could be true but is not based on the background information.\n",
    "Unclear if the answer does not answer the question at all.\n",
    "Question: {{$rag_output}}\n",
    "Background: {{ recall $rag_output }}\n",
    "Answer: {{ $input }}\n",
    "Remember, just answer Grounded or Ungrounded or Unclear: \"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "self_critique_func = kernel.add_function(\n",
    "    function_name=\"self_critique_rag\",\n",
    "    plugin_name=\"RagPlugin\",\n",
    "    prompt=sk_prompt_rag_sc,\n",
    "    prompt_execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(self_critique_func, rag_output=answer, input=answer, chat_history=chat_history)\n",
    "print(f\"The answer was {str(check).strip()}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"   Let's pretend the answer was wrong...\")\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(\n",
    "    self_critique_func, input=answer, rag_output=\"Yes, you live in New York City.\", chat_history=chat_history\n",
    ")\n",
    "print(f\"The answer was {str(check).strip()}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"   Let's pretend the answer is not related...\")\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(\n",
    "    self_critique_func, input=\"Yes, the earth is not flat.\", rag_output=answer, chat_history=chat_history\n",
    ")\n",
    "print(f\"The answer was {str(check).strip()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "In SK, **memory** refers to the storage and recall of information the AI has learned or been provided. There are two primary forms of memory:\n",
    "\n",
    "### Semantic Memory (Long-term)\n",
    "- This is usually an **external vector store** that holds **embeddings of text**, allowing the AI to store facts or documents and later retrieve them by **semantic similarity**.\n",
    "- SK provides **Memory Connectors** to various vector databases (like **Azure Cognitive Search, Pinecone, Qdrant**, etc.) via a common interface.\n",
    "- By using a memory store, you can implement the **retrieval** part of **RAG**: store chunks of knowledge and fetch relevant pieces at query time.\n",
    "- We’ll see how to add and use such memory in our chatbot.\n",
    "\n",
    "### Conversation History (Short-term Memory)\n",
    "- SK also manages the **immediate dialogue context** with a **Chat History object** for multi-turn conversations.\n",
    "- This ensures the AI remembers prior user queries and its own responses, maintaining context across turns.\n",
    "- We will leverage this to keep the conversation coherent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantic-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "# Updated import for memory store in SK 1.23\n",
    "from semantic_kernel.memory import VolatileMemoryStore\n",
    "\n",
    "# Assuming kernel is already set up with chat completion\n",
    "\n",
    "# 1. Set up the embedding service\n",
    "embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Create the embedding service\n",
    "embedding_service = AzureTextEmbedding(\n",
    "    endpoint=base_url,\n",
    "    deployment_name=embedding_deployment_name,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "        endpoint=base_url,    \n",
    "        deployment_name=deployment_name,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Add the embedding service to the kernel\n",
    "kernel = sk.Kernel()\n",
    "kernel.add_service(embedding_service)\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# 2. Create a memory store and semantic memory\n",
    "# Updated to use the correct class from the new location\n",
    "memory_store = VolatileMemoryStore()\n",
    "memory = SemanticTextMemory(storage=memory_store, embeddings_generator=embedding_service)\n",
    "\n",
    "# 3. Save information to memory\n",
    "async def populate_memory():\n",
    "    # Add some facts about programming languages\n",
    "    await memory.save_information(\n",
    "        collection=\"programming\",\n",
    "        id=\"python1\",\n",
    "        text=\"Python is a high-level, interpreted programming language known for its readability and versatility.\"\n",
    "    )\n",
    "    \n",
    "    await memory.save_information(\n",
    "        collection=\"programming\",\n",
    "        id=\"javascript1\",\n",
    "        text=\"JavaScript is a scripting language that enables interactive web pages and is an essential part of web applications.\"\n",
    "    )\n",
    "    \n",
    "    await memory.save_information(\n",
    "        collection=\"programming\",\n",
    "        id=\"csharp1\",\n",
    "        text=\"C# is a modern, object-oriented programming language developed by Microsoft for the .NET platform.\"\n",
    "    )\n",
    "    \n",
    "    # Add some facts about AI and machine learning\n",
    "    await memory.save_information(\n",
    "        collection=\"ai\",\n",
    "        id=\"ml1\",\n",
    "        text=\"Machine learning is a subset of AI that enables systems to learn and improve from experience without being explicitly programmed.\"\n",
    "    )\n",
    "    \n",
    "    await memory.save_information(\n",
    "        collection=\"ai\",\n",
    "        id=\"nlp1\",\n",
    "        text=\"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\"\n",
    "    )\n",
    "    \n",
    "    await memory.save_information(\n",
    "        collection=\"ai\",\n",
    "        id=\"cv1\",\n",
    "        text=\"Computer Vision is an AI field that trains computers to interpret and understand visual information from the world.\"\n",
    "    )\n",
    "    \n",
    "    # Add some facts about Semantic Kernel\n",
    "    await memory.save_information(\n",
    "        collection=\"semantic_kernel\",\n",
    "        id=\"sk1\",\n",
    "        text=\"Semantic Kernel is an open-source SDK that integrates Large Language Models (LLMs) with conventional programming languages.\"\n",
    "    )\n",
    "    \n",
    "    await memory.save_information(\n",
    "        collection=\"semantic_kernel\",\n",
    "        id=\"sk2\",\n",
    "        text=\"Semantic Kernel provides a flexible plugin system that allows developers to extend its functionality.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Memory populated with information.\")\n",
    "\n",
    "# 4. Query the memory\n",
    "async def query_memory():\n",
    "    # Search for information about Python\n",
    "    results = await memory.search(\"programming\", \"Tell me about Python programming\")\n",
    "    print(\"Query: Tell me about Python programming\")\n",
    "    for result in results:\n",
    "        print(f\"Relevance: {result.relevance:.4f}, ID: {result.id}\")\n",
    "        print(f\"Text: {result.text}\\n\")\n",
    "    \n",
    "    # Search for information about AI\n",
    "    results = await memory.search(\"ai\", \"How does natural language processing work?\")\n",
    "    print(\"Query: How does natural language processing work?\")\n",
    "    for result in results:\n",
    "        print(f\"Relevance: {result.relevance:.4f}, ID: {result.id}\")\n",
    "        print(f\"Text: {result.text}\\n\")\n",
    "    \n",
    "    # Search across all collections\n",
    "    results = await memory.search(\"*\", \"What is Semantic Kernel?\")\n",
    "    print(\"Query: What is Semantic Kernel?\")\n",
    "    for result in results:\n",
    "        print(f\"Relevance: {result.relevance:.4f}, Collection: {result.collection}, ID: {result.id}\")\n",
    "        print(f\"Text: {result.text}\\n\")\n",
    "\n",
    "# 5. Build a simple Q&A system\n",
    "async def memory_qa_system():\n",
    "    # Create a semantic function for answering questions\n",
    "    answer_prompt = \"\"\"\n",
    "    Answer the following question based on the provided context.\n",
    "    \n",
    "    Context:\n",
    "    {{$context}}\n",
    "    \n",
    "    Question:\n",
    "    {{$question}}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    answer_fn = kernel.add_function(\n",
    "        prompt=answer_prompt,\n",
    "        function_name=\"answer_with_context\",\n",
    "        plugin_name=\"QA\",\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    async def ask_question(question, collection=\"*\", limit=3):\n",
    "        # Search for relevant information\n",
    "        results = await memory.search(collection, question, limit=limit)\n",
    "        \n",
    "        if not results:\n",
    "            return \"I don't have enough information to answer that question.\"\n",
    "        \n",
    "        # Combine the retrieved information as context\n",
    "        context = \"\\n\".join([f\"- {result.text}\" for result in results])\n",
    "        \n",
    "        # Use the answer function to generate a response\n",
    "        answer = await kernel.invoke(answer_fn, context=context, question=question)\n",
    "        return str(answer)\n",
    "    \n",
    "    # Test the Q&A system with various questions\n",
    "    questions = [\n",
    "        \"What is Python used for?\",\n",
    "        \"Explain the difference between machine learning and computer vision.\",\n",
    "        \"How does Semantic Kernel help developers work with AI?\",\n",
    "        \"What programming language works best with web development?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        answer = await ask_question(question)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "# Run the demonstration\n",
    "async def run_memory_demo():\n",
    "    await populate_memory()\n",
    "    await query_memory()\n",
    "    print(\"\\n--- Q&A System Demo ---\\n\")\n",
    "    await memory_qa_system()\n",
    "\n",
    "# Execute the demo\n",
    "await run_memory_demo()\n",
    "\n",
    "# To run this code, uncomment the line below and execute it in an async context\n",
    "# Example usage:\n",
    "# import asyncio\n",
    "# asyncio.run(run_memory_demo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Semantic Memory** is crucial for building AI apps that retain knowledge or context over time. SK’s memory system allows you to store and retrieve information (text, embeddings, documents) so that the AI can use it later in its reasoning. This is key for scenarios such as:\n",
    "- Recalling past conversations.\n",
    "- Searching a knowledge base (Retrieval-Augmented Generation).\n",
    "- Personalizing responses.\n",
    "\n",
    "Key aspects of SK memory management:\n",
    "\n",
    "- **Embeddings and Vector Stores**: Text is converted into numerical vectors (embeddings) and stored in a vector database (e.g., Azure Cognitive Search, Pinecone, Redis, etc.). This allows you to perform similarity searches at runtime.\n",
    "- **Memory as a Plugin**: Your vector store can be exposed as a plugin function (e.g., `KnowledgePlugin.SearchDocuments(query)`), which the AI can call to retrieve relevant text.\n",
    "- **Storing Memories**: You generate embeddings for text and upsert them into the store along with an identifier and metadata.\n",
    "- **Retrieving Memories**: At query time, you search the memory store using the query’s embedding. The results (relevant text snippets) are then provided to the AI as context.\n",
    "- **Short-term vs Long-term Memory**: SK manages short-term conversational context and long-term semantic memory separately, allowing efficient handling of both.\n",
    "- **Memory Connectors Abstraction**: The updated SK memory interface supports advanced features like multiple embedding vectors per record, metadata filtering, and more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This example demonstrates how the memory search pulls the exact fact from the store and the AI can then use that information to provide a factual answer.\n",
    "\n",
    "\n",
    "**Example of Saving and Retrieving Memory**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In summary, the **Semantic Kernel Intro** section covers the following key points:\n",
    "- An overview of SK and its importance in bridging AI and application code.\n",
    "- Core components like the Kernel, AI services, plugins, context, planner, and memory.\n",
    "- How functions (semantic and native) are organized into plugins.\n",
    "- The power of automatic function calling for multi-step AI reasoning.\n",
    "- The role of filters in ensuring secure and validated execution.\n",
    "- Detailed memory management for integrating external knowledge into AI workflows.\n",
    "\n",
    "This concludes the Intro section in Markdown format with code blocks and dividers for easy copying and use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
