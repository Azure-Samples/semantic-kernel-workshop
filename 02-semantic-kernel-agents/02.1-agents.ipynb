{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Semantic Kernel Agents\n",
    "\n",
    "Welcome to this workshop on Semantic Kernel (SK) agents! In this notebook, we'll explore how to create, configure, and use agents with the Semantic Kernel framework.\n",
    "\n",
    "## What You'll Learn\n",
    "- What agents are in Semantic Kernel\n",
    "- How to create and configure different types of agents\n",
    "- Basic and advanced interaction patterns with agents\n",
    "- How to integrate plugins and functions with agents\n",
    "- Multi-agent systems and orchestration\n",
    "\n",
    "Let's start by setting up our environment and understanding the foundational concepts of Semantic Kernel agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Semantic Kernel Agents\n",
    "\n",
    "### What are agents in Semantic Kernel?\n",
    "\n",
    "In Semantic Kernel, an **agent** is a specialized component that can interact with Large Language Models (LLMs), process conversations, make decisions, and potentially execute code or call functions. Unlike simple prompt-based interactions, agents maintain state, follow instructions, and can engage in multi-turn conversations to accomplish tasks.\n",
    "\n",
    "At their core, agents are designed to:\n",
    "- Process user inputs and generate contextual responses\n",
    "- Maintain conversation history and context\n",
    "- Execute functions when appropriate (function calling)\n",
    "- Work independently or collaborate with other agents\n",
    "\n",
    "### Agent Architecture in Semantic Kernel\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User Input] --> Agent[Agent]\n",
    "    Agent --> K[Kernel]\n",
    "    K --> AI[AI Service]\n",
    "    K --> P[Plugins/Functions]\n",
    "    Agent --> CH[Chat History]\n",
    "    AI --> Agent\n",
    "    P --> Agent\n",
    "    Agent --> Response[Response]\n",
    "```\n",
    "\n",
    "Key components in the agent architecture:\n",
    "\n",
    "1. **Kernel**: The core orchestration engine that connects agents to AI services and functions\n",
    "2. **AI Service**: The underlying LLM that powers the agent (like Azure OpenAI, OpenAI)\n",
    "3. **Chat History**: Maintains the conversation context over multiple turns\n",
    "4. **Plugins/Functions**: Optional extensions that allow the agent to perform specific tasks\n",
    "\n",
    "### Types of Agents in Semantic Kernel\n",
    "\n",
    "Semantic Kernel primarily offers two types of agents:\n",
    "\n",
    "1. **ChatCompletionAgent**\n",
    "   - Lightweight agent that uses your kernel's chat completion service\n",
    "   - Good for simple conversational tasks\n",
    "   - Manages chat history locally\n",
    "\n",
    "2. **OpenAIAssistantAgent / AzureAssistantAgent**\n",
    "   - Uses OpenAI's Assistant API\n",
    "   - Maintains conversation state remotely as \"threads\"\n",
    "   - Supports advanced features like code interpretation and file searching\n",
    "   - Requires explicit thread management\n",
    "\n",
    "Let's first install the necessary packages to work with Semantic Kernel agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install semantic-kernel python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up our environment by loading environment variables and importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('041b03a06ba541a79423aafcc320f79e',\n",
       " 'https://aoai-tst-eastus2.openai.azure.com/',\n",
       " 'gpt-4o-2')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Semantic Kernel components\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv(\"AZURE_OPENAI_API_KEY\"), os.getenv(\"AZURE_OPENAI_ENDPOINT\"), os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we'll need to set up our environment variables with either Azure OpenAI or OpenAI credentials. You can create a `.env` file in the same directory as this notebook with the following variables:\n",
    "\n",
    "```\n",
    "# For Azure OpenAI\n",
    "AZURE_OPENAI_ENDPOINT=\"https://your-resource-name.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"your-azure-openai-api-key\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"your-deployment-name\"\n",
    "\n",
    "# For OpenAI\n",
    "OPENAI_API_KEY=\"your-openai-api-key\"\n",
    "```\n",
    "\n",
    "Let's now create a helper function that will configure a kernel with the appropriate AI service based on the available credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kernel_with_service(service_id=\"default\"):\n",
    "    \"\"\"Create a kernel with either Azure OpenAI or OpenAI service based on available credentials.\"\"\"\n",
    "    kernel = sk.Kernel()\n",
    "    \n",
    "    # Try to use Azure OpenAI first\n",
    "    if os.getenv(\"AZURE_OPENAI_API_KEY\") and os.getenv(\"AZURE_OPENAI_ENDPOINT\") and os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"):\n",
    "        print(\"Using Azure OpenAI service\")\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(\n",
    "                service_id=service_id,\n",
    "                deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "            )\n",
    "        )\n",
    "    # Fall back to OpenAI\n",
    "    elif os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"Using OpenAI service\")\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(\n",
    "                service_id=service_id,\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                ai_model_id=\"gpt-4\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No AI service credentials found. Please set up your .env file.\")\n",
    "    \n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment\n",
    "\n",
    "Now that we understand what agents are and have imported the necessary modules, let's begin by creating our first kernel instance and configuring the environment properly.\n",
    "\n",
    "### Creating a Kernel Instance\n",
    "\n",
    "The `Kernel` is the central orchestration component in Semantic Kernel. It manages AI services, plugins, and other resources that agents need to function. Let's create our first kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Azure OpenAI service\n",
      "Kernel created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create our first kernel with a service ID\n",
    "kernel = create_kernel_with_service(service_id=\"chat-completion\")\n",
    "print(\"Kernel created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Service Configuration\n",
    "\n",
    "When adding an AI service to the kernel, we specified a `service_id`. This ID is important because:\n",
    "\n",
    "1. It allows you to add multiple services to the same kernel\n",
    "2. You can selectively use different services for different agents\n",
    "3. It helps organize and identify your services\n",
    "\n",
    "If you need specific execution settings for your AI service (like temperature, top-p, or function calling behavior), you can retrieve and modify them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service settings updated: Temperature set to 0.7\n"
     ]
    }
   ],
   "source": [
    "# Get the execution settings for our service\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat-completion\")\n",
    "\n",
    "## Set the service settings for the agent\n",
    "using_gpt_4o = True\n",
    "\n",
    "# Example: Adjust the temperature for more creative outputs -- NOTE: RUN THE BELOW ONLY IF YOU ARE USING GPT-4o\n",
    "if using_gpt_4o:\n",
    "    settings.temperature = 0.7\n",
    "    print(f\"Service settings updated: Temperature set to {settings.temperature}\")\n",
    "else: # NOTE: RUN THE BELOW ONLY IF YOU ARE USING o1 or o3-mini\n",
    "    settings.reasoning_effort = \"low\"\n",
    "    print(f\"Service settings updated: Reasoning Effort set to {settings.reasoning_effort}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create Multiple Services in a Kernel\n",
    "\n",
    "In this exercise, try creating a kernel with two different AI services with different configurations. This will be useful when we want to use different services for different agents or functions.\n",
    "\n",
    "Your task:\n",
    "1. Create a new kernel\n",
    "2. Add two services with different service IDs (e.g., \"creative\" and \"precise\")\n",
    "3. Configure the \"creative\" service with higher temperature (e.g., 0.8)\n",
    "4. Configure the \"precise\" service with lower temperature (e.g., 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to create a kernel with multiple services\n",
    "\n",
    "# First, create the kernel\n",
    "\n",
    "# Then add two services with different configurations\n",
    "\n",
    "# Finally, retrieve and modify settings for each service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a new kernel for multiple services\n",
    "multi_service_kernel = sk.Kernel()\n",
    "\n",
    "# Add first service - creative with higher temperature\n",
    "if os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    multi_service_kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=\"creative\",\n",
    "            deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "            endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add second service - precise with lower temperature\n",
    "    multi_service_kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=\"precise\",\n",
    "            deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "            endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        )\n",
    "    )\n",
    "elif os.getenv(\"OPENAI_API_KEY\"):\n",
    "    multi_service_kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=\"creative\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            model_id=\"gpt-4\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    multi_service_kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=\"precise\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            model_id=\"gpt-4\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"No AI service credentials found. Please set up your .env file.\")\n",
    "\n",
    "# Configure the settings for each service\n",
    "creative_settings = multi_service_kernel.get_prompt_execution_settings_from_service_id(service_id=\"creative\")\n",
    "creative_settings.temperature = 0.8\n",
    "print(f\"Creative service temperature: {creative_settings.temperature}\")\n",
    "\n",
    "precise_settings = multi_service_kernel.get_prompt_execution_settings_from_service_id(service_id=\"precise\")\n",
    "precise_settings.temperature = 0.2\n",
    "print(f\"Precise service temperature: {precise_settings.temperature}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "In this section, we've learned how to:\n",
    "\n",
    "1. **Set up our environment** with necessary imports and credentials\n",
    "2. **Create a kernel** as the foundation for our agents\n",
    "3. **Configure AI services** with specific IDs and settings\n",
    "4. **Manage execution settings** to control how the AI generates responses\n",
    "\n",
    "In the next section, we'll create our first agent using the kernel we've just configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your First Agent: ChatCompletionAgent\n",
    "\n",
    "Now that we have our kernel and services set up, let's create our first agent. We'll start with the `ChatCompletionAgent`, which is a simple yet powerful agent that leverages the chat completion capabilities of large language models.\n",
    "\n",
    "### Creating a Basic Agent\n",
    "\n",
    "To create a `ChatCompletionAgent`, we need to provide:\n",
    "1. A kernel with a configured chat service\n",
    "2. Instructions that define the agent's behavior\n",
    "3. Optional parameters like a name and execution settings\n",
    "\n",
    "Let's create a simple assistant agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'Assistant' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a simple assistant agent\n",
    "assistant_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant that provides concise and accurate information. Keep your responses brief but informative.\"\n",
    ")\n",
    "\n",
    "print(f\"Agent '{assistant_agent.name}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Instructions and Parameters\n",
    "\n",
    "The `instructions` parameter is crucial as it defines how your agent will behave. Think of it as the \"system prompt\" that shapes the agent's personality, capabilities, and limitations. Let's explore some more complex instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'MathTutor' created with specialized instructions.\n"
     ]
    }
   ],
   "source": [
    "# Create a specialized agent with detailed instructions\n",
    "math_tutor_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"MathTutor\",\n",
    "    instructions=\"\"\"You are a math tutor specialized in helping students understand mathematical concepts.\n",
    "    \n",
    "    When responding to questions:\n",
    "    1. First explain the underlying concept in simple terms\n",
    "    2. Then walk through the solution step by step\n",
    "    3. Provide a simple example to reinforce the learning\n",
    "    4. Avoid solving problems directly without explanation\n",
    "    \n",
    "    Always be encouraging and patient.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Agent '{math_tutor_agent.name}' created with specialized instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Instructions with Template Parameters\n",
    "\n",
    "You can make your agent's instructions dynamic by including template parameters. These parameters get replaced with actual values when the agent is invoked. This is useful for creating agents that need context that might change, like the current date or user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'TemplatedAgent' created with template parameters.\n"
     ]
    }
   ],
   "source": [
    "# Create an agent with template parameters in instructions\n",
    "templated_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"TemplatedAgent\",\n",
    "    instructions=\"\"\"You are an AI assistant specializing in {{$expertise}}.\n",
    "    Your tone should be {{$tone}} and your responses should be {{$length}} in length.\n",
    "    \n",
    "    Today's date is: {{$current_date}}\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(\n",
    "        expertise=\"Python programming\",\n",
    "        tone=\"friendly\",\n",
    "        length=\"concise\",\n",
    "        current_date=\"2024-03-14\"  # This could be dynamically set when invoking\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Agent '{templated_agent.name}' created with template parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Agent Execution\n",
    "\n",
    "Once an agent is created, it needs a chat history to interact with. The chat history maintains the state of the conversation and provides context for the agent's responses.\n",
    "\n",
    "Here's a simple example of how to execute an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello! I'm an AI assistant here to help with accurate and concise information, answer questions, and provide guidance. Let me know how I can assist you!\n"
     ]
    }
   ],
   "source": [
    "# Create a chat history to maintain conversation state\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# Add a user message to the chat history\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=\"Hello! Can you introduce yourself?\"))\n",
    "\n",
    "# Define a function to execute the agent asynchronously\n",
    "async def get_agent_response(agent, history):\n",
    "    # Get a single response from the agent\n",
    "    response = await agent.get_response(history)\n",
    "    return response\n",
    "\n",
    "# Execute the agent\n",
    "response = await get_agent_response(assistant_agent, chat_history)\n",
    "\n",
    "# Print the agent's response\n",
    "print(f\"Agent: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually three main ways to invoke an agent:\n",
    "\n",
    "1. **`get_response()`**: Returns a single response directly as a `ChatMessageContent` object\n",
    "2. **`invoke()`**: Returns an async iterable of `ChatMessageContent` objects\n",
    "3. **`invoke_stream()`**: Streams the response in real-time (useful for long responses)\n",
    "\n",
    "Let's see how `invoke()` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Quantum computing is a cutting-edge field of computing based on the principles of quantum mechanics....\n"
     ]
    }
   ],
   "source": [
    "# Create a new chat history\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=\"What can you tell me about quantum computing?\"))\n",
    "\n",
    "# Define a function to invoke the agent using invoke()\n",
    "async def invoke_agent(agent, history):\n",
    "    responses = []\n",
    "    # Iterate through the responses asynchronously\n",
    "    async for response in agent.invoke(history):\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "# Execute the agent\n",
    "responses = await invoke_agent(assistant_agent, chat_history)\n",
    "\n",
    "# Print the responses\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}: {response.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a Simple Question-Answering Agent\n",
    "\n",
    "Now it's your turn to create a custom agent. Implement a question-answering agent that specializes in providing factual information about a specific topic of your choice.\n",
    "\n",
    "Your task:\n",
    "1. Create a new `ChatCompletionAgent` with a descriptive name\n",
    "2. Configure it with detailed instructions that define its area of expertise and how it should respond\n",
    "3. Create a chat history with a relevant question\n",
    "4. Execute the agent and display its response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to create and execute a question-answering agent\n",
    "\n",
    "# 1. Create your agent with specialized instructions\n",
    "\n",
    "# 2. Create a chat history with a relevant question\n",
    "\n",
    "# 3. Execute the agent and get its response\n",
    "\n",
    "# 4. Print the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create a specialized question-answering agent\n",
    "space_expert_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"SpaceExpert\",\n",
    "    instructions=\"\"\"You are an expert in astronomy and space exploration.\n",
    "    \n",
    "    When answering questions:\n",
    "    - Provide factual, scientifically accurate information\n",
    "    - Include relevant dates, measurements, and statistics when applicable\n",
    "    - Explain complex concepts in accessible language\n",
    "    - Differentiate between established facts and theoretical or speculative ideas\n",
    "    - When appropriate, mention recent developments or missions\n",
    "    \n",
    "    Focus on being educational and inspiring curiosity about space.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 2. Create a chat history with a relevant question\n",
    "space_chat = ChatHistory()\n",
    "space_chat.add_message(ChatMessageContent(\n",
    "    role=AuthorRole.USER, \n",
    "    content=\"What are exoplanets and how do scientists detect them?\"\n",
    "))\n",
    "\n",
    "# 3. Execute the agent and get its response\n",
    "async def get_expert_response(agent, history):\n",
    "    response = await agent.get_response(history)\n",
    "    return response\n",
    "\n",
    "space_response = await get_expert_response(space_expert_agent, space_chat)\n",
    "\n",
    "# 4. Print the response\n",
    "print(f\"SpaceExpert: {space_response.content}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "In this section, we've learned how to:\n",
    "\n",
    "1. **Create a basic `ChatCompletionAgent`** with specific instructions\n",
    "2. **Configure agent instructions** to define specialized behavior\n",
    "3. **Use template parameters** in instructions for dynamic behavior\n",
    "4. **Execute agents** using different invocation methods\n",
    "\n",
    "In the next section, we'll explore more complex interactions with agents, including multi-turn conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Agent Interactions\n",
    "\n",
    "Now that we've created agents, let's explore how to interact with them in a more conversational way. This involves understanding chat history, adding messages, and processing responses over multiple turns.\n",
    "\n",
    "### Chat History Fundamentals\n",
    "\n",
    "The `ChatHistory` class is fundamental to agent interactions. It:\n",
    "- Maintains the chronological sequence of messages in a conversation\n",
    "- Provides context for the agent to generate relevant responses\n",
    "- Allows for multi-turn conversations with memory of previous exchanges\n",
    "\n",
    "Let's create a new chat history and explore its basic functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n",
      "SYSTEM: This is a conversation between a user and an AI assistant.\n",
      "USER: Hello, I have some questions about programming.\n"
     ]
    }
   ],
   "source": [
    "# Create a new chat history\n",
    "chat = ChatHistory()\n",
    "\n",
    "# Add system message (optional but useful for setting global context)\n",
    "chat.add_message(ChatMessageContent(\n",
    "    role=AuthorRole.SYSTEM,\n",
    "    content=\"This is a conversation between a user and an AI assistant.\"\n",
    "))\n",
    "\n",
    "# Add user message\n",
    "chat.add_message(ChatMessageContent(\n",
    "    role=AuthorRole.USER,\n",
    "    content=\"Hello, I have some questions about programming.\"\n",
    "))\n",
    "\n",
    "# Print the chat history\n",
    "print(\"Chat history:\")\n",
    "for message in chat.messages:\n",
    "    print(f\"{message.role.name}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding User Messages\n",
    "\n",
    "There are a few ways to add user messages to a chat history:\n",
    "\n",
    "1. Using the generic `add_message()` method as shown above\n",
    "2. Using the convenience method `add_user_message()`\n",
    "\n",
    "Let's use the second approach to continue our conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated chat history:\n",
      "SYSTEM: This is a conversation between a user and an AI assistant.\n",
      "USER: Hello, I have some questions about programming.\n",
      "USER: What's the difference between Python and JavaScript?\n"
     ]
    }
   ],
   "source": [
    "# Add a user message using the convenience method\n",
    "chat.add_user_message(\"What's the difference between Python and JavaScript?\")\n",
    "\n",
    "# Print the updated chat history\n",
    "print(\"Updated chat history:\")\n",
    "for message in chat.messages:\n",
    "    print(f\"{message.role.name}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Agent Responses\n",
    "\n",
    "When an agent generates a response, it's important to add it back to the chat history to maintain the conversation flow. Let's see how to do this in a multi-turn conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 1:\n",
      "User: What is Python used for?\n",
      "Agent: Python is used for web development, data analysis, machine learning, artificial intelligence, scientific computing, automation, game development, scripting, and more due to its simplicity and versatility.\n",
      "\n",
      "Turn 2:\n",
      "User: How does it compare to Java?\n",
      "Agent: Python is simpler and more concise, ideal for beginners and rapid development, while Java is robust, faster, and better for large-scale applications. Python emphasizes readability; Java emphasizes performance and structure.\n",
      "\n",
      "Turn 3:\n",
      "User: What would you recommend for a beginner to learn first?\n",
      "Agent: I recommend starting with **Python** for its simplicity, readability, and beginner-friendly syntax. It allows you to focus on learning programming concepts without being overwhelmed by complex syntax.\n"
     ]
    }
   ],
   "source": [
    "# Create a function to handle a multi-turn conversation\n",
    "async def have_conversation(agent, chat_history, user_messages):\n",
    "    \"\"\"Have a multi-turn conversation with an agent.\n",
    "    \n",
    "    Args:\n",
    "        agent: The ChatCompletionAgent to converse with\n",
    "        chat_history: The ChatHistory to use for the conversation\n",
    "        user_messages: A list of strings representing user messages\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, message in enumerate(user_messages):\n",
    "        print(f\"\\nTurn {i+1}:\")\n",
    "        print(f\"User: {message}\")\n",
    "        \n",
    "        # Add the user message to chat history\n",
    "        chat_history.add_user_message(message)\n",
    "        \n",
    "        # Get the agent's response\n",
    "        response = await agent.get_response(chat_history)\n",
    "        \n",
    "        # Add the agent's response to chat history\n",
    "        chat_history.add_message(response)\n",
    "        \n",
    "        print(f\"Agent: {response.content}\")\n",
    "\n",
    "# Create a fresh chat history\n",
    "programming_chat = ChatHistory()\n",
    "\n",
    "# List of user messages for a multi-turn conversation\n",
    "user_messages = [\n",
    "    \"What is Python used for?\",\n",
    "    \"How does it compare to Java?\",\n",
    "    \"What would you recommend for a beginner to learn first?\"\n",
    "]\n",
    "\n",
    "# Create a programming tutor agent\n",
    "programming_tutor = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"ProgrammingTutor\",\n",
    "    instructions=\"You are an experienced programming tutor who explains concepts clearly and concisely. Make your answers very concise.\"\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "await have_conversation(programming_tutor, programming_chat, user_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Iteration with invoke()\n",
    "\n",
    "As we saw earlier, we can also use the `invoke()` method to get responses asynchronously. This is particularly useful when an agent might generate multiple response parts or when you want to process the response incrementally.\n",
    "\n",
    "Let's implement a conversation using `invoke()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to handle a multi-turn conversation using invoke()\n",
    "async def have_conversation_with_invoke(agent, chat_history, user_messages):\n",
    "    \"\"\"Have a multi-turn conversation with an agent using invoke().\n",
    "    \n",
    "    Args:\n",
    "        agent: The ChatCompletionAgent to converse with\n",
    "        chat_history: The ChatHistory to use for the conversation\n",
    "        user_messages: A list of strings representing user messages\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, message in enumerate(user_messages):\n",
    "        print(f\"\\nTurn {i+1}:\")\n",
    "        print(f\"User: {message}\")\n",
    "        \n",
    "        # Add the user message to chat history\n",
    "        chat_history.add_user_message(message)\n",
    "        \n",
    "        # Get the agent's response using invoke()\n",
    "        responses = []\n",
    "        async for response in agent.invoke(chat_history):\n",
    "            responses.append(response)\n",
    "            # In a real app, you might update UI here as responses come in\n",
    "        \n",
    "        \n",
    "        # Add the final response to chat history\n",
    "        if responses:\n",
    "            final_response = responses[-1]\n",
    "            print(f\"Agent: {final_response.content}\")\n",
    "            chat_history.add_message(final_response)\n",
    "            \n",
    "\n",
    "# Create a fresh chat history\n",
    "story_chat = ChatHistory()\n",
    "\n",
    "# List of user messages for a creative conversation\n",
    "creative_messages = [\n",
    "    \"Create a short story about a fox wandering around on Mars. This is a story for kids.\",\n",
    "    \"Now make an alternative ending.\",\n",
    "    \"Can you suggest a title for this story?\"\n",
    "]\n",
    "\n",
    "# Create a storyteller agent\n",
    "storyteller = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"Storyteller\",\n",
    "    instructions=\"You are a creative storyteller who writes engaging short stories with vivid descriptions. Make your stories short in about 2 or 3 paragraphs.\"\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "await have_conversation_with_invoke(storyteller, story_chat, creative_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a Multi-turn Conversation\n",
    "\n",
    "Now it's your turn to build a multi-turn conversation with an agent of your choice. Create an agent with a specific role or expertise, and design a conversation that showcases its capabilities.\n",
    "\n",
    "Your task:\n",
    "1. Create a specialized agent with appropriate instructions\n",
    "2. Design a sequence of 3-5 user messages that form a coherent conversation\n",
    "3. Implement the conversation using either `get_response()` or `invoke()`\n",
    "4. Display the full conversation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to create a multi-turn conversation\n",
    "\n",
    "# 1. Create your specialized agent\n",
    "\n",
    "# 2. Define your user messages\n",
    "\n",
    "# 3. Create a chat history\n",
    "\n",
    "# 4. Implement the conversation\n",
    "\n",
    "# 5. Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create a specialized travel advisor agent\n",
    "travel_advisor = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"TravelAdvisor\",\n",
    "    instructions=\"\"\"You are an experienced travel advisor with knowledge about destinations worldwide.\n",
    "    \n",
    "    When helping travelers:\n",
    "    - Provide specific recommendations based on their interests and constraints\n",
    "    - Include practical details like best times to visit, estimated costs, and local customs\n",
    "    - Suggest off-the-beaten-path experiences when appropriate\n",
    "    - Take into account factors like budget, time of year, and travel style\n",
    "    - Be honest about potential challenges or considerations\n",
    "    \n",
    "    Your goal is to help travelers plan memorable trips that align with their preferences.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 2. Define a sequence of user messages for a cohesive conversation\n",
    "travel_messages = [\n",
    "    \"I'm planning a 10-day trip in June and I'm trying to decide between Japan and Italy. I love culture, food, and beautiful scenery. What would you recommend?\",\n",
    "    \"Japan sounds interesting! What would be an ideal itinerary for a first-time visitor?\",\n",
    "    \"What's the best way to get around Japan? Should I rent a car?\",\n",
    "    \"How much should I budget for this trip, including accommodations, food, and transportation?\",\n",
    "    \"Thanks for all the information! Are there any cultural customs or etiquette I should be aware of?\"\n",
    "]\n",
    "\n",
    "# 3. Create a fresh chat history\n",
    "travel_chat = ChatHistory()\n",
    "\n",
    "# 4. Implement the conversation with the agent\n",
    "async def travel_conversation(agent, chat, messages):\n",
    "    print(\"=== Travel Planning Conversation ===\\n\")\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Traveler: {message}\\n\")\n",
    "        \n",
    "        # Add user message\n",
    "        chat.add_user_message(message)\n",
    "        \n",
    "        # Get agent response\n",
    "        response = await agent.get_response(chat)\n",
    "        \n",
    "        # Add response to history\n",
    "        chat.add_message(response)\n",
    "        \n",
    "        print(f\"Travel Advisor: {response.content}\\n\")\n",
    "    \n",
    "    print(\"=== End of Conversation ===\")\n",
    "\n",
    "# Run the conversation\n",
    "await travel_conversation(travel_advisor, travel_chat, travel_messages)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "In this section, we've learned how to:\n",
    "\n",
    "1. **Use ChatHistory** to maintain the state of a conversation\n",
    "2. **Add messages** from users and agents to the chat history\n",
    "3. **Process agent responses** in both single-turn and multi-turn conversations\n",
    "4. **Use different invocation methods** (`get_response()` and `invoke()`) for agent interaction\n",
    "\n",
    "Understanding these fundamentals of agent interaction prepares us for more advanced topics like integrating plugins and functions, which we'll explore in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Function Calling and Plugins\n",
    "\n",
    "One of the most powerful features of Semantic Kernel agents is their ability to use plugins and call functions. This enables agents to go beyond just generating text and actually perform actions, retrieve information, and integrate with external systems.\n",
    "\n",
    "### What are Plugins in Semantic Kernel?\n",
    "\n",
    "In Semantic Kernel, a **plugin** is a collection of related functions that can be registered with the kernel and made available to agents. Plugins can:\n",
    "- Retrieve information from databases or APIs\n",
    "- Perform calculations or data transformations\n",
    "- Execute system operations\n",
    "- Interact with external services\n",
    "\n",
    "Plugins contain one or more **functions**, which can be:\n",
    "1. **Native Functions**: Written in code (Python) that execute when called\n",
    "2. **Semantic Functions**: Defined by prompts that are sent to the LLM when called\n",
    "\n",
    "Let's create a simple plugin with native functions to demonstrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility plugin registered with functions: ['convert_temperature', 'get_current_time', 'square_root']\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import kernel_function, KernelFunction\n",
    "from typing import Annotated\n",
    "import datetime\n",
    "\n",
    "# Define a simple plugin class with useful functions\n",
    "class UtilityPlugin:\n",
    "    \"\"\"A plugin with utility functions for time, math, and other operations.\"\"\"\n",
    "    \n",
    "    @kernel_function(description=\"Get the current date and time.\")\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"Get the current date and time in ISO format.\"\"\"\n",
    "        return datetime.datetime.now().isoformat()\n",
    "    \n",
    "    @kernel_function(description=\"Calculate the square root of a number.\")\n",
    "    def square_root(self, number: Annotated[float, \"The number to calculate the square root of.\"]) -> str:\n",
    "        \"\"\"Calculate the square root of a given number.\"\"\"\n",
    "        return str(round(number ** 0.5, 4))\n",
    "    \n",
    "    @kernel_function(description=\"Convert temperatures between Celsius and Fahrenheit.\")\n",
    "    def convert_temperature(\n",
    "        self, \n",
    "        temp: Annotated[float, \"The temperature to convert.\"],\n",
    "        unit: Annotated[str, \"The source unit ('C' or 'F')\"]\n",
    "    ) -> str:\n",
    "        \"\"\"Convert temperatures between Celsius and Fahrenheit.\"\"\"\n",
    "        if unit.upper() == 'C':\n",
    "            # Convert from Celsius to Fahrenheit\n",
    "            result = (temp * 9/5) + 32\n",
    "            return f\"{temp}°C is equal to {round(result, 2)}°F\"\n",
    "        elif unit.upper() == 'F':\n",
    "            # Convert from Fahrenheit to Celsius\n",
    "            result = (temp - 32) * 5/9\n",
    "            return f\"{temp}°F is equal to {round(result, 2)}°C\"\n",
    "        else:\n",
    "            return f\"Invalid unit: {unit}. Please use 'C' for Celsius or 'F' for Fahrenheit.\"\n",
    "\n",
    "# Create a new utility plugin instance\n",
    "utility_plugin = UtilityPlugin()\n",
    "\n",
    "# Add the plugin to our kernel\n",
    "kernel.add_plugin(utility_plugin, plugin_name=\"Utility\")\n",
    "\n",
    "print(\"Utility plugin registered with functions:\", \n",
    "      [f for f in kernel.get_plugin(\"Utility\").functions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Plugins with Agents\n",
    "\n",
    "Once we've created a plugin and added it to the kernel, any agent using that kernel can access its functions. However, to enable the agent to automatically call these functions, we need to configure **function calling behavior**.\n",
    "\n",
    "Let's set up an agent that can use our utility plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent 'MathAssistant' with auto function calling enabled\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "# First, get the execution settings for our service\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat-completion\")\n",
    "\n",
    "# Configure automatic function calling\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create an agent with access to our functions\n",
    "math_assistant = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"MathAssistant\",\n",
    "    instructions=\"\"\"You are a helpful assistant that can perform mathematical calculations and utility operations.\n",
    "    Use the available functions when appropriate to provide accurate answers.\n",
    "    For calculations, always use the provided functions rather than calculating yourself.\n",
    "    Make your answers clear and concise.\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(settings=settings) # Pass the settings with auto function calling\n",
    ")\n",
    "\n",
    "print(f\"Created agent '{math_assistant.name}' with auto function calling enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function Calling\n",
    "\n",
    "Now let's test our agent with some questions that would benefit from using the plugin functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What's the square root of 144?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Function call: square_root\n",
      "Arguments: {\"number\":144}\n",
      "Function result: 12.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Agent: The square root of 144 is 12.\n",
      "\n",
      "User: Can you convert 25 degrees Celsius to Fahrenheit?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Function call: convert_temperature\n",
      "Arguments: {\"temp\":25,\"unit\":\"C\"}\n",
      "Function result: 25.0°C is equal to 77.0°F\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Agent: 25 degrees Celsius is equal to 77 degrees Fahrenheit.\n",
      "\n",
      "User: What's the current date and time?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Function call: get_current_time\n",
      "Arguments: {}\n",
      "Function result: 2025-03-14T17:18:08.399233\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Agent: The current date and time is March 14, 2025, at 17:18 (5:18 PM).\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.contents.function_call_content import FunctionCallContent\n",
    "from semantic_kernel.contents.function_result_content import FunctionResultContent\n",
    "\n",
    "\n",
    "\n",
    "# Create a function that shows function calls and results\n",
    "async def demonstrate_function_calling(agent, history, question):\n",
    "    print(f\"\\nUser: {question}\")\n",
    "    history.add_user_message(question)\n",
    "    \n",
    "    # Use invoke to see the full process including function calls\n",
    "    function_calls = []\n",
    "    function_results = []\n",
    "    \n",
    "    async for response in agent.invoke(history):\n",
    "        final_response = response\n",
    "\n",
    "    for response in function_chat:\n",
    "        # Check if this is a function call        \n",
    "        for item in response.items:\n",
    "            if isinstance(item, FunctionCallContent):\n",
    "                function_call = item\n",
    "                function_calls.append(function_call)\n",
    "                print(f\"\\n{80*'-'}\")\n",
    "                print(f\"Function call: {function_call.function_name}\")\n",
    "                print(f\"Arguments: {function_call.arguments}\")\n",
    "            elif isinstance(item, FunctionResultContent):\n",
    "                function_result = item\n",
    "                function_results.append(function_result)\n",
    "                print(f\"Function result: {function_result.result}\")\n",
    "                print(80*\"-\")\n",
    "        \n",
    "    # Get the final answer separately for clarity\n",
    "    # final_response = await agent.get_response(history)\n",
    "    history.add_message(final_response)\n",
    "    \n",
    "    print(f\"\\nAgent: {final_response.content}\")\n",
    "    \n",
    "    return {\n",
    "        \"function_calls\": function_calls,\n",
    "        \"function_results\": function_results,\n",
    "        \"final_response\": final_response\n",
    "    }\n",
    "\n",
    "# Test with different questions\n",
    "questions = [\n",
    "    \"What's the square root of 144?\",\n",
    "    \"Can you convert 25 degrees Celsius to Fahrenheit?\",\n",
    "    \"What's the current date and time?\"\n",
    "]\n",
    "\n",
    "# Test each question\n",
    "for question in questions:\n",
    "    # Create a chat history\n",
    "    function_chat = ChatHistory()\n",
    "    await demonstrate_function_calling(math_assistant, function_chat, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling Modes\n",
    "\n",
    "Semantic Kernel supports different modes for function calling:\n",
    "\n",
    "1. **Auto**: The agent automatically decides whether to call functions based on the context.\n",
    "2. **RequireFunction**: The agent must call at least one function in its response.\n",
    "3. **Disabled**: The agent cannot call functions; it must respond using only text.\n",
    "\n",
    "Let's see how to configure these different modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing AutoAgent ---\n",
      "Question: What's the square root of 169?\n",
      "Function called: square_root\n",
      "Response: The square root of 169 is 13.\n",
      "Function called: True\n",
      "\n",
      "--- Testing RequireAgent ---\n",
      "Question: What's the square root of 169?\n",
      "Function called: square_root\n",
      "Response: The square root of 169 is 13.\n",
      "Function called: True\n",
      "\n",
      "--- Testing DisabledAgent ---\n",
      "Question: What's the square root of 169?\n",
      "Response: The square root of 169 is 13.\n",
      "Function called: False\n"
     ]
    }
   ],
   "source": [
    "# Function to create an agent with a specific function calling mode\n",
    "def create_agent_with_mode(mode, name):\n",
    "    settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat-completion\")\n",
    "    \n",
    "    if mode == \"auto\":\n",
    "        settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "    elif mode == \"require\":\n",
    "        settings.function_choice_behavior = FunctionChoiceBehavior.Required()\n",
    "    elif mode == \"disabled\":\n",
    "        settings.function_choice_behavior = FunctionChoiceBehavior.NoneInvoke()\n",
    "    \n",
    "    return ChatCompletionAgent(\n",
    "        kernel=kernel,\n",
    "        name=name,\n",
    "        instructions=\"\"\"You are a helpful assistant that can perform mathematical calculations and utility operations.\n",
    "        Answer questions concisely and accurately.\n",
    "        \"\"\",\n",
    "        arguments=KernelArguments(settings=settings)\n",
    "    )\n",
    "\n",
    "# Create agents with different modes\n",
    "auto_agent = create_agent_with_mode(\"auto\", \"AutoAgent\")\n",
    "require_agent = create_agent_with_mode(\"require\", \"RequireAgent\")\n",
    "disabled_agent = create_agent_with_mode(\"disabled\", \"DisabledAgent\")\n",
    "\n",
    "# Function to test the agents with a simple question\n",
    "async def test_function_mode(agent, question):\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(question)\n",
    "    \n",
    "    print(f\"\\n--- Testing {agent.name} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    try:\n",
    "        # Use invoke to see if function calls occur\n",
    "        function_called = False\n",
    "        async for response in agent.invoke(chat):\n",
    "            final_response = response\n",
    "        \n",
    "        for response in chat:\n",
    "            for item in response.items:\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    function_called = True\n",
    "                    print(f\"Function called: {item.function_name}\")\n",
    "        \n",
    "        # Get the final response\n",
    "        print(f\"Response: {final_response.content[:100]}...\" if len(final_response.content) > 100 else f\"Response: {final_response.content}\")\n",
    "        print(f\"Function called: {function_called}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test all agents with a math question\n",
    "question = \"What's the square root of 169?\"\n",
    "await test_function_mode(auto_agent, question)\n",
    "await test_function_mode(require_agent, question)\n",
    "await test_function_mode(disabled_agent, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a More Complex Plugin\n",
    "\n",
    "Let's create a more practical plugin that simulates a weather service to see how agents can work with more complex data and APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather plugin registered with functions: ['get_forecast', 'get_weather']\n"
     ]
    }
   ],
   "source": [
    "class WeatherPlugin:\n",
    "    \"\"\"A plugin that simulates weather data retrieval.\"\"\"\n",
    "    \n",
    "    # Simulated weather data for demonstration\n",
    "    _weather_data = {\n",
    "        \"new york\": {\"temperature\": 22, \"condition\": \"Partly Cloudy\", \"humidity\": 65},\n",
    "        \"london\": {\"temperature\": 18, \"condition\": \"Rainy\", \"humidity\": 80},\n",
    "        \"tokyo\": {\"temperature\": 28, \"condition\": \"Sunny\", \"humidity\": 70},\n",
    "        \"paris\": {\"temperature\": 24, \"condition\": \"Clear\", \"humidity\": 60},\n",
    "        \"sydney\": {\"temperature\": 31, \"condition\": \"Hot\", \"humidity\": 55},\n",
    "    }\n",
    "    \n",
    "    @kernel_function(description=\"Get the current weather for a city.\")\n",
    "    def get_weather(\n",
    "        self, \n",
    "        city: Annotated[str, \"The name of the city to get weather for.\"]\n",
    "    ) -> str:\n",
    "        \"\"\"Get the current weather conditions for a specific city.\"\"\"\n",
    "        city_lower = city.lower()\n",
    "        \n",
    "        if city_lower in self._weather_data:\n",
    "            data = self._weather_data[city_lower]\n",
    "            return f\"Weather in {city.title()}: {data['temperature']}°C, {data['condition']}, Humidity: {data['humidity']}%\"\n",
    "        else:\n",
    "            return f\"Weather data not available for {city}.\"\n",
    "    \n",
    "    @kernel_function(description=\"Get the forecast for a city for the next few days.\")\n",
    "    def get_forecast(\n",
    "        self, \n",
    "        city: Annotated[str, \"The name of the city to get forecast for.\"],\n",
    "        days: Annotated[int, \"Number of days in the forecast (1-5).\"] = 3\n",
    "    ) -> str:\n",
    "        \"\"\"Get a weather forecast for a specific city for the next few days.\"\"\"\n",
    "        city_lower = city.lower()\n",
    "        \n",
    "        if city_lower not in self._weather_data:\n",
    "            return f\"Forecast data not available for {city}.\"\n",
    "        \n",
    "        if days < 1 or days > 5:\n",
    "            return \"Please request a forecast between 1 and 5 days.\"\n",
    "        \n",
    "        # Create a simulated forecast based on current conditions\n",
    "        import random\n",
    "        current = self._weather_data[city_lower]\n",
    "        forecast = []\n",
    "        \n",
    "        conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Rainy\", \"Clear\"]\n",
    "        \n",
    "        for i in range(days):\n",
    "            # Randomly vary the temperature within a reasonable range\n",
    "            temp_change = random.uniform(-3, 3)\n",
    "            new_temp = round(current[\"temperature\"] + temp_change)\n",
    "            \n",
    "            # Pick a condition (with some continuity from current)\n",
    "            if i == 0:\n",
    "                condition = current[\"condition\"]\n",
    "            else:\n",
    "                condition = random.choice(conditions)\n",
    "                \n",
    "            # Add to forecast\n",
    "            day_name = [\"Today\", \"Tomorrow\", \"Day 3\", \"Day 4\", \"Day 5\"][i]\n",
    "            forecast.append(f\"{day_name}: {new_temp}°C, {condition}\")\n",
    "        \n",
    "        result = f\"Weather forecast for {city.title()}:\\n\"\n",
    "        result += \"\\n\".join(forecast)\n",
    "        return result\n",
    "\n",
    "# Create a weather plugin instance and add it to the kernel\n",
    "weather_plugin = WeatherPlugin()\n",
    "kernel.add_plugin(weather_plugin, plugin_name=\"Weather\")\n",
    "\n",
    "print(\"Weather plugin registered with functions:\", \n",
    "      [f for f in kernel.get_plugin(\"Weather\").functions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Agent with Multiple Plugins\n",
    "\n",
    "Now let's create an agent that has access to both our utility and weather plugins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent 'TravelAssistant' with access to multiple plugins\n"
     ]
    }
   ],
   "source": [
    "# Create settings with auto function calling\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat-completion\")\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a travel assistant agent with access to all plugins\n",
    "travel_assistant = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"TravelAssistant\",\n",
    "    instructions=\"\"\"You are a helpful travel assistant that helps users plan their trips.\n",
    "    Use the available functions to provide weather information and perform calculations when needed.\n",
    "    \n",
    "    When responding to questions about destinations:\n",
    "    1. Provide weather information when relevant\n",
    "    2. Suggest appropriate activities based on the weather\n",
    "    3. Help with temperature conversions if needed\n",
    "    4. Be specific and helpful in your recommendations\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(settings=settings)\n",
    ")\n",
    "\n",
    "print(f\"Created agent '{travel_assistant.name}' with access to multiple plugins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Complex Queries\n",
    "\n",
    "Let's test our travel assistant with some more complex queries that might require using multiple functions or combining function results with general knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: I'm planning a trip to Tokyo next week. What's the weather like and what should I pack?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Travel Assistant: Next week in Tokyo, the weather looks pleasant, with temperatures ranging from 26°C to 29°C. Here's the breakdown:\n",
      "\n",
      "- **Today**: 27°C, Sunny\n",
      "- **Tomorrow**: 26°C, Partly Cloudy\n",
      "- **Day 3**: 29°C, Cloudy\n",
      "- **Day 4**: 26°C, Clear\n",
      "- **Day 5**: 28°C, Partly Cloudy\n",
      "\n",
      "### Packing Suggestions:\n",
      "1. **Clothing**: Lightweight clothing like T-shirts, shorts, or breathable dresses will be comfortable in this warm weather. Pack a light jacket or sweater for cooler evenings.\n",
      "2. **Accessories**: Sunglasses and sunscreen are essential for sunny days. Consider a hat for additional sun protection.\n",
      "3. **Shoes**: Comfortable walking shoes are great for exploring the city.\n",
      "4. **Umbrella**: Just in case there's unexpected rain under the cloudy skies.\n",
      "\n",
      "Let me know if you'd like temperature conversion to Fahrenheit or activity recommendations!\n",
      "\n",
      "Functions called:\n",
      "- get_forecast({\"city\":\"Tokyo\",\"days\":5})\n",
      "\n",
      "User: If it's 28°C in Tokyo, what is that in Fahrenheit? And how does that compare to New York?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Travel Assistant: In Tokyo, 28°C is equal to **82.4°F**.\n",
      "\n",
      "In comparison, the current weather in New York is **22°C (71.6°F)**, partly cloudy, with 65% humidity.\n",
      "\n",
      "Tokyo is warmer than New York right now, so you'll likely find Tokyo more summery compared to New York's milder conditions. Let me know if you need suggestions based on these conditions!\n",
      "\n",
      "Functions called:\n",
      "- get_forecast({\"city\":\"Tokyo\",\"days\":5})\n",
      "- convert_temperature({\"temp\": 28, \"unit\": \"C\"})\n",
      "- get_weather({\"city\": \"New York\"})\n"
     ]
    }
   ],
   "source": [
    "# Create a function to test the travel assistant\n",
    "async def ask_travel_assistant(question, chat):\n",
    "    chat.add_user_message(question)\n",
    "    \n",
    "    print(f\"\\nUser: {question}\")\n",
    "    print(\"\\nProcessing...\")\n",
    "    \n",
    "    # Track function calls\n",
    "    function_calls = []\n",
    "    \n",
    "    # Stream the response to see it build up\n",
    "    print(\"\\nTravel Assistant: \", end=\"\")\n",
    "    async for response in travel_assistant.invoke(chat):\n",
    "        final_response = response\n",
    "\n",
    "    # Get the final response\n",
    "    for response in chat:\n",
    "        # Check if this is a function call\n",
    "        for item in response.items:\n",
    "            if isinstance(item, FunctionCallContent):\n",
    "                function_calls.append({\n",
    "                    \"name\": item.function_name,\n",
    "                    \"arguments\": item.arguments\n",
    "                })\n",
    "    \n",
    "    print(f\"{final_response.content}\")\n",
    "    \n",
    "    # Add this response to chat history\n",
    "    chat.add_message(final_response)\n",
    "    \n",
    "    # Print a summary of function calls\n",
    "    if function_calls:\n",
    "        print(\"\\nFunctions called:\")\n",
    "        for call in function_calls:\n",
    "            print(f\"- {call['name']}({call['arguments']})\")\n",
    "    else:\n",
    "        print(\"\\nNo functions were called.\")\n",
    "    \n",
    "    return chat\n",
    "\n",
    "chat = ChatHistory()\n",
    "\n",
    "# Test with complex questions\n",
    "travel_chat = await ask_travel_assistant(\"I'm planning a trip to Tokyo next week. What's the weather like and what should I pack?\", chat)\n",
    "\n",
    "# Ask a follow-up question that might use multiple plugins\n",
    "travel_chat = await ask_travel_assistant(\"If it's 28°C in Tokyo, what is that in Fahrenheit? And how does that compare to New York?\", chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create Your Own Plugin\n",
    "\n",
    "Now it's your turn to create a custom plugin and use it with an agent. Your plugin could simulate a database lookup, a translation service, or any other useful functionality.\n",
    "\n",
    "Your task:\n",
    "1. Create a custom plugin class with at least two kernel functions\n",
    "2. Add the plugin to the kernel\n",
    "3. Create an agent that uses your plugin\n",
    "4. Test the agent with appropriate queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to create a custom plugin\n",
    "\n",
    "# 1. Define your plugin class with kernel functions\n",
    "\n",
    "# 2. Create an instance and add it to the kernel\n",
    "\n",
    "# 3. Create an agent that can use your plugin\n",
    "\n",
    "# 4. Test the agent with appropriate queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "class TranslationPlugin:\n",
    "    \"\"\"A plugin that simulates a language translation service.\"\"\"\n",
    "    \n",
    "    # Dictionary of supported languages and their codes\n",
    "    _supported_languages = {\n",
    "        \"english\": \"en\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"french\": \"fr\",\n",
    "        \"german\": \"de\",\n",
    "        \"italian\": \"it\",\n",
    "        \"japanese\": \"ja\",\n",
    "        \"chinese\": \"zh\",\n",
    "    }\n",
    "    \n",
    "    # Simple translation dictionary for common phrases (in a real plugin, this would use an API)\n",
    "    _translations = {\n",
    "        \"hello\": {\n",
    "            \"es\": \"hola\",\n",
    "            \"fr\": \"bonjour\",\n",
    "            \"de\": \"hallo\",\n",
    "            \"it\": \"ciao\",\n",
    "            \"ja\": \"こんにちは\",\n",
    "            \"zh\": \"你好\"\n",
    "        },\n",
    "        \"goodbye\": {\n",
    "            \"es\": \"adiós\",\n",
    "            \"fr\": \"au revoir\",\n",
    "            \"de\": \"auf wiedersehen\",\n",
    "            \"it\": \"arrivederci\",\n",
    "            \"ja\": \"さようなら\",\n",
    "            \"zh\": \"再见\"\n",
    "        },\n",
    "        \"thank you\": {\n",
    "            \"es\": \"gracias\",\n",
    "            \"fr\": \"merci\",\n",
    "            \"de\": \"danke\",\n",
    "            \"it\": \"grazie\",\n",
    "            \"ja\": \"ありがとう\",\n",
    "            \"zh\": \"谢谢\"\n",
    "        },\n",
    "        \"please\": {\n",
    "            \"es\": \"por favor\",\n",
    "            \"fr\": \"s'il vous plaît\",\n",
    "            \"de\": \"bitte\",\n",
    "            \"it\": \"per favore\",\n",
    "            \"ja\": \"お願いします\",\n",
    "            \"zh\": \"请\"\n",
    "        },\n",
    "        \"how are you\": {\n",
    "            \"es\": \"¿cómo estás?\",\n",
    "            \"fr\": \"comment allez-vous?\",\n",
    "            \"de\": \"wie geht es dir?\",\n",
    "            \"it\": \"come stai?\",\n",
    "            \"ja\": \"お元気ですか？\",\n",
    "            \"zh\": \"你好吗？\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @kernel_function(description=\"Get a list of supported languages for translation.\")\n",
    "    def list_languages(self) -> str:\n",
    "        \"\"\"Return a list of languages supported by the translation service.\"\"\"\n",
    "        languages = list(self._supported_languages.keys())\n",
    "        return f\"Supported languages: {', '.join(languages)}\"\n",
    "    \n",
    "    @kernel_function(description=\"Translate a phrase from English to another language.\")\n",
    "    def translate(\n",
    "        self, \n",
    "        text: Annotated[str, \"The English text to translate.\"],\n",
    "        target_language: Annotated[str, \"The language to translate to.\"]\n",
    "    ) -> str:\n",
    "        \"\"\"Translate the given English text to the specified target language.\"\"\"\n",
    "        # Convert to lowercase for matching\n",
    "        text_lower = text.lower()\n",
    "        language_lower = target_language.lower()\n",
    "        \n",
    "        # Check if the target language is supported\n",
    "        if language_lower not in self._supported_languages:\n",
    "            return f\"Sorry, translation to {target_language} is not supported. Use list_languages() to see supported languages.\"\n",
    "        \n",
    "        # Get the language code\n",
    "        lang_code = self._supported_languages[language_lower]\n",
    "        \n",
    "        # Check if we have a translation for this phrase\n",
    "        for phrase, translations in self._translations.items():\n",
    "            if phrase in text_lower:\n",
    "                if lang_code in translations:\n",
    "                    # Replace the phrase with its translation\n",
    "                    translated = text.lower().replace(phrase, translations[lang_code])\n",
    "                    return f\"Translation to {target_language}: {translated}\"\n",
    "        \n",
    "        # For phrases we don't have stored, we'll simulate a generic response\n",
    "        return f\"Translation to {target_language} would normally be provided through an external API.\"\n",
    "    \n",
    "    @kernel_function(description=\"Detect the language of a given text.\")\n",
    "    def detect_language(self, text: Annotated[str, \"The text to analyze.\"]) -> str:\n",
    "        \"\"\"Detect the likely language of the provided text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Simple detection based on known translations\n",
    "        for phrase, translations in self._translations.items():\n",
    "            for lang_code, translated_phrase in translations.items():\n",
    "                if translated_phrase in text_lower:\n",
    "                    # Get the language name from the code\n",
    "                    language = next(name for name, code in self._supported_languages.items() if code == lang_code)\n",
    "                    return f\"Detected language: {language.capitalize()}\"\n",
    "        \n",
    "        # Default response if no match found\n",
    "        return \"Language detection would normally use detailed analysis through an external API.\"\n",
    "\n",
    "# Create the translation plugin\n",
    "translation_plugin = TranslationPlugin()\n",
    "kernel.add_plugin(translation_plugin, plugin_name=\"Translation\")\n",
    "\n",
    "# Create settings with auto function calling\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat-completion\")\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a language assistant\n",
    "language_assistant = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"LanguageAssistant\",\n",
    "    instructions=\"\"\"You are a helpful language assistant that can help users with translations.\n",
    "    Use the available functions to provide translations and language information.\n",
    "    \n",
    "    When answering questions:\n",
    "    1. If a user wants to know what languages are supported, use the list_languages function\n",
    "    2. If a user wants a translation, use the translate function\n",
    "    3. If a user provides text in a foreign language, try to identify it with detect_language\n",
    "    4. Provide cultural context when relevant to the translation\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(settings=settings)\n",
    ")\n",
    "\n",
    "# Test the language assistant\n",
    "async def test_language_assistant():\n",
    "    chat = ChatHistory()\n",
    "    \n",
    "    # Test questions\n",
    "    questions = [\n",
    "        \"What languages can you translate to?\",\n",
    "        \"How do I say 'thank you' in French?\",\n",
    "        \"Can you detect what language this is: 'Grazie mille'?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nUser: {question}\")\n",
    "        chat.add_user_message(question)\n",
    "        \n",
    "        response = await language_assistant.get_response(chat)\n",
    "        chat.add_message(response)\n",
    "        \n",
    "        print(f\"Language Assistant: {response.content}\")\n",
    "\n",
    "# Run the test\n",
    "await test_language_assistant()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "In this section, we've learned how to:\n",
    "\n",
    "1. **Create plugins** with native functions to extend agent capabilities\n",
    "2. **Register plugins** with the kernel so agents can access them\n",
    "3. **Configure function calling behavior** to enable automatic function invocation\n",
    "4. **Use multiple plugins** together to create more powerful agents\n",
    "5. **Track function calls and their results** for debugging and auditing\n",
    "\n",
    "Plugins and function calling are what transform agents from simple chat bots into powerful tools that can interact with data, perform calculations, and integrate with external systems. In the next section, we'll explore how to create agents with more specialized patterns and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single Agent Patterns\n",
    "\n",
    "Now that we've explored the basics of agents and how to extend them with plugins, let's dive into some more advanced patterns for working with single agents. These patterns will help you create more dynamic, responsive, and robust agent implementations.\n",
    "\n",
    "### Streaming Responses\n",
    "\n",
    "For long responses or scenarios where you want to show progress in real-time, streaming is essential. Semantic Kernel's `invoke_stream()` method allows you to receive agent responses incrementally as they're generated, rather than waiting for the complete response.\n",
    "\n",
    "Let's implement an agent that generates creative content and see how streaming works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Write a short story about a traveler who discovers a hidden village in the mountains where time flows differently.\n",
      "\n",
      "CreativeWriter (streaming): The wind whispered secrets through the towering pines as Elara, backpack weighed heavy with stories yet untold, followed the faint trail etched into the mountainside. The sky above her, awash in hues of dawn, seemed frozen in perpetual motion, a surreal golden hour that refused to yield to the march of time. Rumors had carried her here—half-jests in smoky taverns and cryptic footnotes in forgotten manuscripts—that spoke of a hidden village ensconced in the mountains' embrace, where the world’s clock ticked to a peculiar rhythm.\n",
      "\n",
      "As she crested the ridge, the village revealed itself like an apparition. Cradled in the valley below, it glittered with an ethereal light, its rooftops crafted from silvered wood and its winding paths paved with stones that shimmered faintly like moonlight captured in solid form. She descended cautiously, her breath catching as the air seemed thicker, sweeter, laced with an unplaceable scent—like summer rain mingling with centuries-old parchment.\n",
      "\n",
      "The people of the village greeted her with neither surprise nor suspicion, but with warm, knowing smiles. Their eyes held tranquil pools of depth, their faces marked not by age but by stories etched into their expressions. Here, she was told, time flowed not in minutes or hours but in emotions, memories, and connections. A single day might linger endlessly if filled with wonder, while a year could vanish in the blink of an eye if spent in monotony. Elara marveled; the village was not trapped but liberated, unshackled from the linear relentlessness of conventional time.\n",
      "\n",
      "She stayed longer than she intended—how long exactly, she couldn’t say. In this place, she rediscovered the art of savoring moments. She painted murals on the side of ancient homes, exchanged tales beneath the boughs of a glowing willow, and danced with villagers at twilight festivals that seemed to stretch into infinity. But one evening, beneath a sky frozen in perpetual dusk, she knew her time had come to leave. As she climbed the ridge to depart, she realized with a pang that the life she had lived within those shimmering borders felt richer than the decades outside. Yet, when she finally descended into the realm of clocks and calendars, the journey that had reshaped her felt surreal, as though it had lasted only a fleeting heartbeat.\n",
      "\n",
      "Streaming stats:\n",
      "- Generated 2312 characters\n",
      "- Took 10.65 seconds\n",
      "- Average speed: 217.1 characters/second\n"
     ]
    }
   ],
   "source": [
    "# Create a creative writing agent\n",
    "creative_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"CreativeWriter\",\n",
    "    instructions=\"\"\"You are a creative writer who can generate vivid and engaging content.\n",
    "    When asked to write something, create detailed and descriptive content that brings the subject to life.\n",
    "    Use rich imagery, metaphors, and sensory details to make your writing immersive.\n",
    "    Always be original and imaginative in your storytelling. Generate content that are a maximum of 4 paragraphs long.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Function to demonstrate streaming responses\n",
    "async def demonstrate_streaming(agent, prompt):\n",
    "    \"\"\"Show a streaming response with live updates.\"\"\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(prompt)\n",
    "    \n",
    "    print(f\"User: {prompt}\\n\")\n",
    "    print(\"CreativeWriter (streaming): \", end=\"\")\n",
    "    \n",
    "    # Use invoke_stream to get response chunks as they're generated\n",
    "    response_chunks = []\n",
    "    character_count = 0\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Stream the response\n",
    "        async for chunk in agent.invoke_stream(chat):\n",
    "            if chunk.content:\n",
    "                response_chunks.append(chunk.content)\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                character_count += len(chunk.content)\n",
    "        \n",
    "        # Print some analytics about the streaming\n",
    "        end_time = datetime.datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        print(f\"\\n\\nStreaming stats:\")\n",
    "        print(f\"- Generated {character_count} characters\")\n",
    "        print(f\"- Took {duration:.2f} seconds\")\n",
    "        print(f\"- Average speed: {character_count/duration:.1f} characters/second\")\n",
    "        \n",
    "        # Return the concatenated response\n",
    "        return \"\".join(response_chunks)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nError during streaming: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Try with a creative writing prompt\n",
    "writing_prompt = \"Write a short story about a traveler who discovers a hidden village in the mountains where time flows differently.\"\n",
    "story = await demonstrate_streaming(creative_agent, writing_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Streaming vs. Non-Streaming\n",
    "\n",
    "Streaming is particularly beneficial for:\n",
    "- Providing immediate feedback to users\n",
    "- Showing progress for long-running generations\n",
    "- Building more responsive interfaces\n",
    "- Enhancing the conversational feel of agent interactions\n",
    "\n",
    "However, it does add complexity to your code, as you need to handle and concatenate chunks. Let's compare the streaming approach with the non-streaming approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING APPROACH:\n",
      "User: Describe a serene mountain lake at sunrise in one paragraph.\n",
      "\n",
      "CreativeWriter (streaming): The mountain lake is a mirror of tranquility, cradled in the embrace of towering peaks cloaked in the soft hues of dawn. The water, still and glass-like, holds the first light of the day like liquid gold, rippling gently as a whispering breeze journeys across its surface. Wisps of mist hover above, ethereal and fleeting, dissolving as the sun breaks the horizon, painting the sky in washes of peach and lavender. Pine trees line the edges of the shore, their silhouettes stark yet graceful against the glowing horizon, while the faint songs of birds stir the silence, weaving a melody into the morning’s repose. It is a sacred tableau of nature’s artistry, where time slows and every breath feels like an invitation to peace.\n",
      "\n",
      "Streaming stats:\n",
      "- Generated 727 characters\n",
      "- Took 3.52 seconds\n",
      "- Average speed: 206.4 characters/second\n",
      "\n",
      "\n",
      "NON-STREAMING APPROACH:\n",
      "\n",
      "User: Describe a serene mountain lake at sunrise in one paragraph.\n",
      "\n",
      "Generating response (waiting for full completion)... Done! (2.45 seconds)\n",
      "\n",
      "CreativeWriter (non-streaming): The mountain lake shimmers like a giant mirror cradled in the arms of jagged peaks, reflecting the delicate hues of dawn. The water is so still it seems as though time holds its breath, catching the blush of soft pinks and fiery oranges that stretch across the sky. Mist curls in ethereal wisps above the surface, whispering secrets to the towering evergreens that guard the lake’s edge. The air is crisp and tinged with the scent of pine and earth, carrying the faint echoes of a distant bird calling to its mate. Sunlight begins its gentle descent, spilling golden streams onto the rippling surface, coaxing the world awake in a harmony of color, sound, and silence.\n",
      "\n",
      "\n",
      "COMPARISON:\n",
      "1. Streaming: User sees partial results immediately\n",
      "2. Non-streaming: User waits until the entire response is ready\n",
      "3. Both produce the same final content\n"
     ]
    }
   ],
   "source": [
    "# Let's compare streaming vs. non-streaming for a shorter prompt\n",
    "comparison_prompt = \"Describe a serene mountain lake at sunrise in one paragraph.\"\n",
    "\n",
    "# First with streaming\n",
    "print(\"STREAMING APPROACH:\")\n",
    "streaming_result = await demonstrate_streaming(creative_agent, comparison_prompt)\n",
    "\n",
    "# Now with non-streaming for comparison\n",
    "async def demonstrate_non_streaming(agent, prompt):\n",
    "    \"\"\"Show a non-streaming response.\"\"\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(prompt)\n",
    "    \n",
    "    print(f\"\\nUser: {prompt}\\n\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"Generating response (waiting for full completion)...\", end=\"\")\n",
    "    \n",
    "    # Get the response all at once\n",
    "    response = await agent.get_response(chat)\n",
    "    \n",
    "    # Calculate timing\n",
    "    end_time = datetime.datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\" Done! ({duration:.2f} seconds)\\n\")\n",
    "    print(f\"CreativeWriter (non-streaming): {response.content}\")\n",
    "    \n",
    "    # Return the response content\n",
    "    return response.content\n",
    "\n",
    "print(\"\\n\\nNON-STREAMING APPROACH:\")\n",
    "non_streaming_result = await demonstrate_non_streaming(creative_agent, comparison_prompt)\n",
    "\n",
    "# Compare both approaches\n",
    "print(\"\\n\\nCOMPARISON:\")\n",
    "print(f\"1. Streaming: User sees partial results immediately\")\n",
    "print(f\"2. Non-streaming: User waits until the entire response is ready\")\n",
    "print(f\"3. Both produce the same final content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Instructions with Template Parameters\n",
    "\n",
    "We've already seen a basic example of using template parameters in agent instructions, but let's explore this concept in more depth. Template parameters in instructions allow you to create flexible agents that can adapt to different contexts or requirements without redefining the agent.\n",
    "\n",
    "Key benefits of template parameters:\n",
    "- Create reusable agent templates\n",
    "- Inject context at runtime\n",
    "- Customize agent behavior without changing code\n",
    "- Pass dynamic information like date/time, user preferences, or conversation context\n",
    "\n",
    "Let's create a versatile agent that uses multiple template parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DynamicAgent with template parameters\n",
      "\n",
      "==== AGENT CONFIGURATION ====\n",
      "current_date: 2025-03-14\n",
      "domain: nutrition and dietetics\n",
      "tone: friendly\n",
      "length: concise\n",
      "detail_level: high\n",
      "user_name: Sarah\n",
      "user_preferences: Vegetarian, interested in sustainable food sources\n",
      "============================\n",
      "\n",
      "User: Tell me about healthy eating habits.\n",
      "\n",
      "DynamicAgent: Healthy eating habits ensure you get the nutrients needed for optimal health and well-being. Here are some practical tips:\n",
      "\n",
      "### 1. **Prioritize Whole Foods**\n",
      "   - Focus on fruits, vegetables, whole grains, legumes, nuts, and seeds. Whole, minimally processed foods are rich in nutrients and fiber.\n",
      "   - For vegetarians like you, include protein sources like lentils, chickpeas, tofu, tempeh, and quinoa.\n",
      "\n",
      "### 2. **Balance Your Plate**\n",
      "   - Aim for a mix of macronutrients at every meal: carbohydrates (whole grains), protein (plant-based sources), and healthy fats (avocado, olive oil, nuts).\n",
      "   - Use the \"plate method\": half your plate should be vegetables, one-quarter protein, and one-quarter whole grains.\n",
      "\n",
      "### 3. **Watch Portion Sizes**\n",
      "   - Even healthy foods can lead to weight gain if eaten in excessive amounts. Use smaller plates and pay attention to hunger and fullness cues.\n",
      "\n",
      "### 4. **Stay Hydrated**\n",
      "   - Drink plenty of water throughout the day. Herbal teas are also a great option if you want variety.\n",
      "\n",
      "### 5. **Limit Processed and Sugary Foods**\n",
      "   - Reduce consumption of packaged snacks, sugary drinks, and refined carbs. These often contain unhealthy fats and added sugars with little nutritional value.\n",
      "\n",
      "### 6. **Incorporate Plant-Based Diversity**\n",
      "   - Diversity in your diet helps ensure a range of vitamins and minerals are covered. Include a variety of colorful vegetables and fruits to \"eat the rainbow.\"\n",
      "\n",
      "### 7. **Mindful Eating**\n",
      "   - Eat slowly, chew thoroughly, and avoid distractions like watching TV while eating. This helps prevent overeating and aids in digestion.\n",
      "\n",
      "### 8. **Sustainability Considerations**\n",
      "   - You already focus on sustainability! Incorporate locally grown, seasonal produce and reduce food waste by planning meals and repurposing leftovers.\n",
      "\n",
      "### 9. **Prep and Plan**\n",
      "   - Prepare meals in advance to avoid relying on takeout or heavily processed ready meals. Batch cooking is especially useful for busy weeks.\n",
      "\n",
      "### 10. **Snack Smart**\n",
      "   - Choose nutrient-dense snacks like roasted chickpeas, mixed nuts, or fresh fruit.\n",
      "\n",
      "### 11. **Fortified Foods**\n",
      "   - For vegetarians, seek fortified plant milks or cereals for nutrients like B12, calcium, and vitamin D if needed.\n",
      "\n",
      "### 12. **Moderation, Not Deprivation**\n",
      "   - It’s okay to enjoy indulgent foods occasionally. Balance is key, not perfection.\n",
      "\n",
      "Would you like personalized tips for meal planning or specific nutrients to focus on for a vegetarian diet?\n",
      "\n",
      "\n",
      "==== AGENT CONFIGURATION ====\n",
      "current_date: 2025-03-14\n",
      "domain: literary analysis\n",
      "tone: academic\n",
      "formality: formal\n",
      "detail_level: extensive\n",
      "primary_goal: provide a scholarly analysis with examples\n",
      "user_name: Professor Johnson\n",
      "context: This is for an advanced English literature course\n",
      "============================\n",
      "\n",
      "User: What's the significance of Hemingway's writing style?\n",
      "\n",
      "DynamicAgent: Ernest Hemingway’s writing style is both distinctive and influential, characterized by precision, economy of language, and what he referred to as the \"Iceberg Theory,\" or the theory of omission. His approach to writing revolutionized modern literature and remains a touchstone for discussions about how structure and stylistic choices contribute to meaning and thematic depth.\n",
      "\n",
      "### 1. **Economy of Language:**\n",
      "Hemingway is renowned for his sparse, unadorned prose style, relying on short sentences and eliminating unnecessary words. This minimalist technique strips writing of embellishment, creating a powerful immediacy and focus. For example, in *The Old Man and the Sea*, Hemingway writes:\n",
      "\n",
      "> \"He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish.\"\n",
      "\n",
      "This opening sentence exemplifies his ability to convey substantive information using straightforward syntax. The plain language emphasizes the stark, almost existential nature of the protagonist’s struggle. Readers are left to infer complexity from the simplicity of the words.\n",
      "\n",
      "### 2. **Iceberg Theory:**\n",
      "Hemingway described his style as operating on the principle of omission, likening his works to an iceberg: seven-eighths of the substance lies beneath the surface. Rather than explaining everything explicitly, he leaves much unsaid, trusting readers to deduce meaning from implication. This approach creates a layered depth and engages readers in active interpretation.\n",
      "\n",
      "For example, in his short story “Hills Like White Elephants,” the dialogue between the male and female characters regarding an abortion remains elliptical. The word “abortion” is never mentioned, yet the tension is palpable through subtext, pauses, and fragmented communication. Readers are tasked with piecing together the emotional stakes and significance.\n",
      "\n",
      "### 3. **Focus on Action Rather Than Emotion:**\n",
      "Hemingway's prose often foregrounds action over emotional exposition, allowing the characters’ behavior and speech to reveal their inner lives. This style aligns with his emphasis on stoicism and resilience, themes prominent in his work. In *A Farewell to Arms*, the horrors of war are conveyed through detached description of events rather than explicit moralizing or sentimental language:\n",
      "\n",
      "> “At the start of the winter came the permanent rain and with the rain came cholera. But it was checked and in the end only seven thousand died of it in the army.”\n",
      "\n",
      "The understated presentation of mass death avoids melodrama, lending gravity and realism to the text. Readers find emotional resonance less in overt declarations than in the starkness of events.\n",
      "\n",
      "### 4. **Impact on Narrative Voice:**\n",
      "Hemingway's style aligns closely with the narrative voice he employs—a near journalistic neutrality tinged with introspection. The stripped-down prose often creates a sense of immediacy and objectivity, which works particularly well in settings of war, nature, and survival. This neutrality further reinforces themes of existentialism and the human condition.\n",
      "\n",
      "Hemingway’s writing style profoundly influenced modern literature, setting a precedent for restraint and subtlety in storytelling. His innovations in minimalism force readers to grapple with subtext and ambiguity, creating an intellectually engaging experience. As a result, his style is not merely a hallmark of craft but serves as a thematic extension of the content of his work: human endurance, the complexities of relationships, and the persistently enigmatic nature of existence.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ernest Hemingway’s writing style is both distinctive and influential, characterized by precision, economy of language, and what he referred to as the \"Iceberg Theory,\" or the theory of omission. His approach to writing revolutionized modern literature and remains a touchstone for discussions about how structure and stylistic choices contribute to meaning and thematic depth.\\n\\n### 1. **Economy of Language:**\\nHemingway is renowned for his sparse, unadorned prose style, relying on short sentences and eliminating unnecessary words. This minimalist technique strips writing of embellishment, creating a powerful immediacy and focus. For example, in *The Old Man and the Sea*, Hemingway writes:\\n\\n> \"He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish.\"\\n\\nThis opening sentence exemplifies his ability to convey substantive information using straightforward syntax. The plain language emphasizes the stark, almost existential nature of the protagonist’s struggle. Readers are left to infer complexity from the simplicity of the words.\\n\\n### 2. **Iceberg Theory:**\\nHemingway described his style as operating on the principle of omission, likening his works to an iceberg: seven-eighths of the substance lies beneath the surface. Rather than explaining everything explicitly, he leaves much unsaid, trusting readers to deduce meaning from implication. This approach creates a layered depth and engages readers in active interpretation.\\n\\nFor example, in his short story “Hills Like White Elephants,” the dialogue between the male and female characters regarding an abortion remains elliptical. The word “abortion” is never mentioned, yet the tension is palpable through subtext, pauses, and fragmented communication. Readers are tasked with piecing together the emotional stakes and significance.\\n\\n### 3. **Focus on Action Rather Than Emotion:**\\nHemingway\\'s prose often foregrounds action over emotional exposition, allowing the characters’ behavior and speech to reveal their inner lives. This style aligns with his emphasis on stoicism and resilience, themes prominent in his work. In *A Farewell to Arms*, the horrors of war are conveyed through detached description of events rather than explicit moralizing or sentimental language:\\n\\n> “At the start of the winter came the permanent rain and with the rain came cholera. But it was checked and in the end only seven thousand died of it in the army.”\\n\\nThe understated presentation of mass death avoids melodrama, lending gravity and realism to the text. Readers find emotional resonance less in overt declarations than in the starkness of events.\\n\\n### 4. **Impact on Narrative Voice:**\\nHemingway\\'s style aligns closely with the narrative voice he employs—a near journalistic neutrality tinged with introspection. The stripped-down prose often creates a sense of immediacy and objectivity, which works particularly well in settings of war, nature, and survival. This neutrality further reinforces themes of existentialism and the human condition.\\n\\nHemingway’s writing style profoundly influenced modern literature, setting a precedent for restraint and subtlety in storytelling. His innovations in minimalism force readers to grapple with subtext and ambiguity, creating an intellectually engaging experience. As a result, his style is not merely a hallmark of craft but serves as a thematic extension of the content of his work: human endurance, the complexities of relationships, and the persistently enigmatic nature of existence.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a highly customizable agent with template parameters\n",
    "dynamic_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"DynamicAgent\",\n",
    "    instructions=\"\"\"You are an AI assistant specializing in {{$domain}}.\n",
    "    \n",
    "    Your primary goal is to {{$primary_goal}}.\n",
    "    \n",
    "    When responding:\n",
    "    - Use a {{$tone}} tone\n",
    "    - Keep responses {{$length}} in length\n",
    "    - Use {{$formality}} language\n",
    "    - Include {{$detail_level}} level of detail\n",
    "    \n",
    "    Today is {{$current_date}}.\n",
    "    User's name: {{$user_name}}\n",
    "    User's preferences: {{$user_preferences}}\n",
    "    \n",
    "    Additional context: {{$context}}\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(\n",
    "        # Default values that can be overridden at invocation time\n",
    "        domain=\"general knowledge\",\n",
    "        primary_goal=\"provide helpful and accurate information\",\n",
    "        tone=\"neutral\",\n",
    "        length=\"moderate\",\n",
    "        formality=\"semi-formal\",\n",
    "        detail_level=\"medium\",\n",
    "        current_date=datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        user_name=\"User\",\n",
    "        user_preferences=\"No specific preferences\",\n",
    "        context=\"No additional context provided\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Created {dynamic_agent.name} with template parameters\")\n",
    "\n",
    "# Function to demonstrate the versatility of template parameters\n",
    "async def use_dynamic_agent(question, **kwargs):\n",
    "    \"\"\"Use the dynamic agent with custom parameters for each invocation.\"\"\"\n",
    "    # Create a chat history\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(question)\n",
    "    \n",
    "    # Create arguments with default values\n",
    "    args = KernelArguments(\n",
    "        current_date=datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "    \n",
    "    # Update with any provided keyword arguments\n",
    "    args.update(kwargs)\n",
    "    \n",
    "    # Print the configuration being used\n",
    "    print(\"\\n==== AGENT CONFIGURATION ====\")\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"============================\\n\")\n",
    "    \n",
    "    print(f\"User: {question}\\n\")\n",
    "    \n",
    "    # Get response with the custom parameters\n",
    "    response = await dynamic_agent.get_response(chat, arguments=args)\n",
    "    \n",
    "    print(f\"DynamicAgent: {response.content}\\n\")\n",
    "    return response.content\n",
    "\n",
    "# Test the agent with different configurations\n",
    "await use_dynamic_agent(\n",
    "    \"Tell me about healthy eating habits.\",\n",
    "    domain=\"nutrition and dietetics\",\n",
    "    tone=\"friendly\",\n",
    "    length=\"concise\",\n",
    "    detail_level=\"high\",\n",
    "    user_name=\"Sarah\",\n",
    "    user_preferences=\"Vegetarian, interested in sustainable food sources\"\n",
    ")\n",
    "\n",
    "await use_dynamic_agent(\n",
    "    \"What's the significance of Hemingway's writing style?\",\n",
    "    domain=\"literary analysis\",\n",
    "    tone=\"academic\",\n",
    "    formality=\"formal\",\n",
    "    detail_level=\"extensive\",\n",
    "    primary_goal=\"provide a scholarly analysis with examples\",\n",
    "    user_name=\"Professor Johnson\",\n",
    "    context=\"This is for an advanced English literature course\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling Basics\n",
    "\n",
    "When working with agents, various errors can occur:\n",
    "- Network issues when calling the AI service\n",
    "- Invalid inputs or parameters\n",
    "- Rate limiting or quota exhaustion\n",
    "- Function execution errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Error Handling\n",
    "\n",
    "When implementing error handling for agents, consider these best practices:\n",
    "\n",
    "1. **Categorize errors** by type for appropriate responses:\n",
    "   - Transient errors (network issues, rate limits) → retry with backoff\n",
    "   - User errors (invalid inputs) → return clear error messages\n",
    "   - System errors (configuration issues) → log and alert\n",
    "\n",
    "2. **Implement retry logic** with:\n",
    "   - Exponential backoff (wait longer between retries)\n",
    "   - Maximum retry limits\n",
    "   - Jitter (random variation in wait times) for distributed systems\n",
    "\n",
    "3. **Graceful degradation**:\n",
    "   - Have fallback responses ready\n",
    "   - Consider simplifying requests that are too complex\n",
    "   - Offer alternative paths when a service is unavailable\n",
    "\n",
    "4. **Comprehensive logging**:\n",
    "   - Track all errors for later analysis\n",
    "   - Include context about user inputs, conversation state\n",
    "   - Correlate errors with specific requests\n",
    "\n",
    "5. **User communication**:\n",
    "   - Provide clear, helpful error messages\n",
    "   - Avoid exposing technical details to end users\n",
    "   - Set appropriate expectations for retries or alternatives\n",
    "\n",
    "These practices ensure your agent implementations are resilient and provide a good user experience even when things go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build an Agent with Dynamic Instructions\n",
    "\n",
    "Now it's your turn to apply what you've learned. Create an agent that combines dynamic template parameters. The agent should be able to adapt its personality based on user preferences and handle potential errors gracefully.\n",
    "\n",
    "Your task:\n",
    "1. Create an agent with template parameters for personality, expertise, and response style\n",
    "2. Implement a function that allows the user to reconfigure the agent's parameters at runtime\n",
    "3. Test the agent with different configurations and scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to create a dynamic agent with error handling\n",
    "\n",
    "# 1. Create an agent with template parameters\n",
    "\n",
    "# 2. Implement a function for reconfiguring the agent\n",
    "\n",
    "# 3. Test the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create an agent with template parameters\n",
    "adaptive_agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"AdaptiveAgent\",\n",
    "    instructions=\"\"\"You are an adaptive AI assistant with the following characteristics:\n",
    "    \n",
    "    Personality: {{$personality}}\n",
    "    Expertise: {{$expertise}}\n",
    "    Verbosity: {{$verbosity}}\n",
    "    Formality: {{$formality}}\n",
    "    Humor Level: {{$humor}}\n",
    "    \n",
    "    When responding to the user:\n",
    "    - Answer in a way that matches the personality traits above\n",
    "    - Prioritize accuracy in your {{$expertise}} knowledge domain\n",
    "    - Keep responses {{$verbosity}} in length\n",
    "    - Maintain a {{$formality}} tone throughout\n",
    "    - Include humor that is {{$humor}} when appropriate\n",
    "    \n",
    "    Additional notes: {{$additional_instructions}}\n",
    "    \"\"\",\n",
    "    arguments=KernelArguments(\n",
    "        # Default configuration\n",
    "        personality=\"friendly and helpful\",\n",
    "        expertise=\"general knowledge\",\n",
    "        verbosity=\"moderate\",\n",
    "        formality=\"conversational\",\n",
    "        humor=\"mild\",\n",
    "        additional_instructions=\"Be respectful and inclusive at all times.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Implement a function for reconfiguring and using the agent with error handling\n",
    "async def use_adaptive_agent(question, config=None, max_retries=3):\n",
    "    \"\"\"Use the adaptive agent with optional reconfiguration and error handling.\"\"\"\n",
    "    \n",
    "    # Start with default arguments\n",
    "    args = KernelArguments(\n",
    "        personality=\"friendly and helpful\",\n",
    "        expertise=\"general knowledge\",\n",
    "        verbosity=\"moderate\",\n",
    "        formality=\"conversational\",\n",
    "        humor=\"mild\",\n",
    "        additional_instructions=\"Be respectful and inclusive at all times.\"\n",
    "    )\n",
    "    \n",
    "    # Update with provided configuration if any\n",
    "    if config and isinstance(config, dict):\n",
    "        for key, value in config.items():\n",
    "            if key in args:\n",
    "                args[key] = value\n",
    "    \n",
    "    # Create chat history\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(question)\n",
    "    \n",
    "    # Print current configuration\n",
    "    print(\"\\n==== AGENT CONFIGURATION ====\")\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"============================\\n\")\n",
    "    \n",
    "    print(f\"User: {question}\\n\")\n",
    "    \n",
    "    # Attempt to get response with the current configuration\n",
    "    response = await adaptive_agent.get_response(chat, arguments=args)\n",
    "    \n",
    "    # Success!\n",
    "    print(f\"AdaptiveAgent: {response.content}\\n\")\n",
    "    return response.content\n",
    "    return f\"Sorry, I encountered a problem: {last_error}\"\n",
    "\n",
    "# 3. Test the agent with different configurations\n",
    "# Test with default configuration\n",
    "await use_adaptive_agent(\"What's your approach to answering questions?\")\n",
    "\n",
    "# Test with custom configuration - Technical expert\n",
    "tech_config = {\n",
    "    \"personality\": \"analytical and precise\",\n",
    "    \"expertise\": \"computer programming and technology\",\n",
    "    \"verbosity\": \"verbose\",\n",
    "    \"formality\": \"professional\",\n",
    "    \"humor\": \"occasional\",\n",
    "    \"additional_instructions\": \"Include code examples when relevant.\"\n",
    "}\n",
    "await use_adaptive_agent(\"How do web browsers render HTML and CSS?\", tech_config)\n",
    "\n",
    "# Test with another configuration - Brief casual advisor\n",
    "casual_config = {\n",
    "    \"personality\": \"casual and approachable\",\n",
    "    \"expertise\": \"everyday advice\",\n",
    "    \"verbosity\": \"brief\",\n",
    "    \"formality\": \"informal\",\n",
    "    \"humor\": \"frequent\",\n",
    "    \"additional_instructions\": \"Use conversational language and relatable examples.\"\n",
    "}\n",
    "await use_adaptive_agent(\"What's a good way to start a conversation at a networking event?\", casual_config)\n",
    "\n",
    "# 4. Test error handling by intentionally triggering an error and recovery\n",
    "# This configuration has an invalid verbosity level that would be corrected during error recovery\n",
    "invalid_config = {\n",
    "    \"verbosity\": \"super-extensive\",  # Invalid value that might cause issues\n",
    "    \"expertise\": \"error handling\"\n",
    "}\n",
    "await use_adaptive_agent(\"Tell me a detailed explanation of quantum computing.\", invalid_config)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "In this section, we've learned about advanced patterns for working with single agents:\n",
    "\n",
    "1. **Streaming Responses**\n",
    "   - Using `invoke_stream()` to get real-time updates from agents\n",
    "   - Comparing streaming vs. non-streaming approaches\n",
    "   - Managing streaming output for improved user experience\n",
    "\n",
    "2. **Dynamic Instructions with Template Parameters**\n",
    "   - Creating versatile agents that adapt to different contexts\n",
    "   - Passing runtime parameters to customize agent behavior\n",
    "   - Implementing reusable agent templates with different configurations\n",
    "\n",
    "3. **Error Handling Best Practices**\n",
    "   - Categorizing and handling different types of errors\n",
    "   - Implementing retry logic with exponential backoff\n",
    "   - Creating robust agent interactions that gracefully handle issues\n",
    "   - Providing meaningful feedback when errors occur\n",
    "\n",
    "These advanced patterns help you create more sophisticated, adaptable, and resilient agents that can provide better user experiences in a variety of scenarios.\n",
    "\n",
    "In the next sections, we'll build on these foundations to explore multi-agent systems, where multiple specialized agents collaborate to accomplish complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
